# Técnicas de Memória/Contexto além da RAG em Agentes LLM de Geração de Código

LLMs para programação enfrentam o desafio de trabalhar com **contexto extenso e dinâmico** – por exemplo, múltiplos arquivos de um projeto ou histórico de conversas. Como esses modelos não possuem memória persistente interna, é preciso empregar técnicas externas para fornecer contexto adicional ou simular memória entre requisições. A seguir, discutimos métodos tradicionais e emergentes, detalhando propósito, funcionamento, vantagens, limitações e exemplos práticos.

## Recuperação Ampliada (RAG) para Código (RACG)

**Descrição:** RAG (Retrieval-Augmented Generation) estende o contexto do modelo buscando informações externas relevantes antes da geração. Em *RAG para código* (RACG), extrai-se trechos de código, documentações ou anotações relevantes de uma base de conhecimento (por exemplo, repositórios de código ou documentação técnica) e os combina com a instrução do usuário para a geração de código. Na prática, um componente *retriever* consulta um banco de dados vetorial ou índice, recupera os trechos mais similares à consulta (vetorizando a consulta e comparando vetores) e injeta esses trechos no prompt do LLM. Assim, o LLM “vê” contexto adicional ao gerar código.

* **Propósito:** Superar a limitação do contexto fixo do modelo, incorporando conhecimento especializado (ex.: APIs, padrões de projeto, código pré-existente) no prompt de geração. Para agentes de código, isso equivale a copiar trechos úteis de outros arquivos ou documentação relacionada.
* **Funcionamento:** Um pipeline típico (Figura 8 do levantamento de Jiang et al. 2024) é: 1) *Query* do usuário → 2) *Retriever* vetorial busca documentos/trechos relevantes → 3) Concatena-se o(s) trecho(s) recuperado(s) à consulta → 4) LLM gera o código final. Exemplos de sistemas de pesquisa são Faiss, Elasticsearch ou bases vetoriais customizadas. Em *código*, o retriever pode indexar todo o repositório ou documentação técnica (p.ex. PyPI docs).
* **Vantagens:** Aumenta a qualidade e precisão do código gerado ao trazer exemplos concretos ou definições de APIs. Modelos RAG geralmente cometem menos erros de conceito, pois baseiam-se em fontes de verdade externas. Permite cobrir conhecimento fora do corte de conhecimento do modelo e criar contexto “ilimitado” via memória externa.
* **Limitações:** A eficácia depende da qualidade da recuperação. Se o retriever achar trechos irrelevantes ou desatualizados, o resultado piora. Além disso, requer maior esforço computacional (indexação e busca vetorial) e aumenta o tamanho do prompt, o que consome tokens e pode causar excesso de contexto. Há risco de *overfitting* aos códigos recuperados – o LLM pode copiar literais sem generalizar, levando a violações de licenças ou falta de adaptação às especificidades do pedido. Integrações RAG de código ainda são emergentes, exigindo ajustes finos de busca e formatação do prompt.
* **Exemplos:** Em pesquisa, foram propostos esquemas como **ReACC** (Lu et al. 2022), que copia trechos similares e referencía semanticamente código recuperado, melhorando benchmarks de CodeXGLUE. O **DocPrompting** (Zhou et al. 2022) recupera primeiro documentação relevante e a mistura ao pedido para gerar código baseado em docs. O **RepoCoder** (Zhang et al. 2023) faz geração iterativa em nível de repositório, usando analogias de código entre arquivos. Em produtos comerciais, o *GitHub Copilot* já utiliza algo parecido: ele escaneia arquivos abertos e dependências para enriquecer o contexto do prompt. O *Google Gemini Code Assist* por sua vez tem consciência de projeto inteiro (contexto de até 1 milhão de tokens) e pode integrar repositórios privados para sugerir código ainda mais personalizado.

## Memória Persistente (Long-Term Memory)

**Descrição:** Como LLMs são *estateless*, toda “memória” perceptível vem de inserir manualmente informações prévias no prompt. Memória persistente refere-se ao armazenamento externo de informações relevantes de interações passadas, preferências do usuário ou estados de projeto. Em vez de ter apenas contexto de sessão, o sistema grava fatos ou dados (ex.: nome de variáveis-chave, detalhes de requisitos ou código gerado anteriormente) em um banco de memória. Posteriormente, recupera-se esses dados para continuar conversas ou manter consistência. Técnicas incluem bases vetoriais de “memórias” que armazenam embeddings de tópicos-chave, grafos de conhecimento ou bancos de dados documentais vinculados ao assistente.

* **Propósito:** Manter contexto **entre sessões e chamadas**. Por exemplo, lembrar estilo de código do usuário, bibliotecas preferidas ou problemas já discutidos. Isso torna a interação cumulativa e personalizada, evitando repetição de informação e melhorando a coerência temporal.
* **Funcionamento:** Ao fim de cada interação ou período, o sistema extrai as informações importantes (por exemplo, sumariza ou vetora a última resposta e armazena num índice). Na próxima vez, o LLM consulta essa memória (via vetorização de entrada/consulta) para recuperar e incluir históricos relevantes no prompt. Pode-se usar *llm-context protocol*, onde memórias altas são transformadas em mensagens de sistema invisíveis ou carregadas como contexto inicial.
* **Vantagens:** Garante continuidade em projetos de longo prazo (mantendo estado de variáveis, preferências). Diminui a necessidade de repetir informações (“setup”): o usuário não precisa re-explicar regras ou dados já fornecidos. Em sistemas corporativos, memórias persistentes permitem ao assistente se adaptar ao domínio ou código legado da empresa, lembrando peculiaridades específicas. Isso aumenta produtividade em manutenção de código ou suporte continuado.
* **Limitações:** Implementar memória requer infraestrutura extra (bases vetoriais, bases de dados). Há riscos de privacidade e segurança ao armazenar informações sensíveis. A gestão de memória (o que guardar e o que descartar) é complexa; sem esquecimento inteligente, pode acumular ruído e confundir o LLM, fazendo-o “lembrar” incorretamente de coisas irrelevantes. Além disso, sistemas atuais têm memória inconsistente – alguns detalhes são lembrados, outros perdidos. Memórias mal recuperadas (relevância baixa) podem introduzir mais confusão do que ajudar.
* **Exemplos:** Embora não haja ainda produtos de código dedicados com memória ativa, vemos conceitos em assistentes genéricos: ChatGPT (por exemplo) permite definir *memórias de sistema* e planeja funcionalidades de longo prazo para guardar preferências. Pesquisas como **MemGPT** propõem LLMs com camadas de memória externa para lidar com contextos extensos sem limite de tokens. Em ambientes de programação, um sistema poderia memorizar bugs frequentes de um projeto e avisar preventivamente quando aparecem. Em projetos de IA, bibliotecas como Mem0 ou Pinecone são usadas para implementar esse tipo de memória em assistentes.

## Arquiteturas Híbridas e Agentes Especializados

**Descrição:** Em vez de um único LLM tratando tudo, arquiteturas híbridas dividem a tarefa entre **múltiplos agentes ou módulos**, cada um especializado. Por exemplo, um “roteador de intenções” classifica a consulta, redireciona a tarefas específicas e monta workflows de agentes; agentes distintos geram código, analisam bugs, documentam; um agente executa ou testa código gerado; e há módulos auxiliares para busca, testes unitários ou gráficos de conhecimento. Alguns projetos propõem “super-agentes” compostos por: *Intent Router*, *Task Agents* com RAG e memória, e *Model Router* que escolhe quais LLMs usar (grandes para raciocínio, pequenos para tarefas simples). Essa abordagem reflete micro-serviços cognitivos: cada agente pode ter seu próprio LLM (ou versão) e base de conhecimento especializada.

* **Propósito:** Especializar competências para obter melhor performance e escalabilidade. Em vez de “tudo-em-um”, cada módulo foca numa subtarefa (geração de código, explicação, avaliação de segurança etc.), permitindo usar modelos diferentes (menores para tarefas corriqueiras, maiores para problemas complexos) e integrar ferramentas externas (compiladores, debuggers) em cada etapa.
* **Funcionamento:** Normalmente há um agente orquestrador (*coach* ou *triage*) que recebe a instrução do usuário, divide em sub-tarefas (p.ex. “escreva teste”, “gere função X”) e encaminha a agentes específicos. Cada *agente de tarefa* pode usar RAG (por exemplo, um agente de código com um grafo de conhecimento do projeto), ou até integrar ferramentas: um agente compilador, um executor Python embutido, etc. Os resultados são reunidos pelo roteador e apresentados ao usuário. Em alguns designs, o LLM também atua iterativamente, revisando ou depurando saídas passadas.
* **Vantagens:** Permite **capacidade modular** e paralela. Um agente especializado pode ter profundo conhecimento (ex.: um agente dedicado a frameworks web, outro a APIs do sistema operacional). Atualizações em um módulo não afetam os demais, facilitando manutenção. Possibilita usar recursos localmente (modelos on‑device) para certas tarefas e servidor na nuvem para outras, equilibrando custo e privacidade. Exemplos mostram maior taxa de sugestões corretas ao combinar agentes e RAG.
* **Limitações:** A complexidade do sistema cresce muito. Exige um mecanismo robusto de roteamento e protocolos entre agentes. Há desafio em coordenar respostas de agentes múltiplos de forma coesa, evitando respostas conflituosas. Cada módulo especializado pode falhar isoladamente e é preciso lidar com credenciamento/segurança entre eles. Em termos de LLM, orquestrações multi-modelo ainda são experimentais para código; há poucas soluções prontas.
* **Exemplos:** Entre pesquisas recentes, **Rahul Kumar et al.** descrevem uma arquitetura multi-agentes para software (com triagem, agentes de código, de gráficos, etc.), onde um *Code Generation Agent* usa um *Graph RAG Engine* para respostas precisas. Na prática, assistentes como **AutoGPT** ou **AgentGPT** (não específicos de código) já usam pipelines que repetem chamadas a LLMs, e projetos como **AutoCodeRover** (Zhang et al. 2024) e **SWE-Agent** usam agentes LLM para depuração e manutenção de código. No âmbito comercial, o próprio Copilot Chat introduziu “perfis de domínio” e RAG para tarefas específicas (por exemplo, contexto Cloud do Azure), aproximando-se desse modelo híbrido.

## Supervisão Iterativa e Refinamento de Saída

**Descrição:** Em vez de pedir ao LLM que responda de uma só vez, técnicas iterativas fazem o modelo gerar uma saída parcial, avaliar e refinar repetidamente. *Self-Refine* (Madaan et al. 2023) é um exemplo: o modelo gera uma versão inicial do código, depois re-entra como avaliador, apontando erros ou pontos a melhorar, e pede ao LLM uma versão refinada. O processo repete até satisfazer critérios de qualidade. Outro método relacionado é o uso de *coT* (chain-of-thought) especializado: primeiro, pede-se ao LLM para explicitar o raciocínio ou esboçar passos estruturados do algoritmo (como sequências, laços, condicionais) antes de escrever o código final. O objetivo é simular supervisão humana por múltiplas iterações.

* **Propósito:** Melhorar a qualidade do resultado final e detectar erros lógicos. Em geração de código, isso equivale a *depurar no próprio prompt*: o LLM verifica variáveis, insere testes mentais ou segue uma sequência de passos antes de concluir.
* **Funcionamento:** Tipicamente há dois passes (ou mais): 1) **Geração inicial:** LLM cria código ou solução bruta. 2) **Avaliação/Feedback:** O mesmo modelo (ou um segundo) revê a saída, detecta inconsistências ou falhas (pode executar testes embutidos) e sugere correções. 3) **Refinamento:** O LLM re-escreve o código considerando o feedback. Essa sequência pode se repetir iterativamente. No prompting, isso pode ser feito com instruções como “Explique passo-a-passo seu código” (CoT) ou “Reveja seu próprio código e corrija erros”.
* **Vantagens:** Mesmo sem alterar pesos ou dados de treinamento, consegue elevar performance usando *“pensamento reflexivo”*. Estudos mostram que LLMs baseados em GPT-4, usando Self-Refine, chegam a \~20% de ganho médio em tarefas diversas em comparação à geração única. Para código, estruturar o prompt com raciocínios intermediários (Structured CoT) pode aumentar em até \~13% métricas como Pass\@1. Em suma, permite ao modelo “aprender com seus erros” durante a própria geração.
* **Limitações:** Cada iteração consome tokens e tempo extra (pode ficar lento). Não há garantia de convergência – o modelo pode refinar indefinidamente sem melhorar. Além disso, sem avaliação externa rigorosa (por exemplo, execução de testes reais), o feedback do próprio LLM pode ser incorreto ou raso. Essa abordagem não substitui treinamento formal; os ganhos têm limite e dependem de boa formulação de prompt e paciência do usuário. Em código crítico, confiar apenas na autoavaliação é arriscado – testes reais ou revisão humana continuam sendo necessários.
* **Exemplos:** Tecnologias de CoT personalizado têm sido aplicadas em geradores de código. Li et al. (2023) propuseram o *Structured CoT*, fazendo o LLM pensar em termos de laços/condicionais antes de codificar, melhorando resultados em benchmarks de programação. Em prática comercial, funcionalidades de “Code Interpreter” ou chatbots de codificação (que podem executar e corrigir código) usam esse princípio: geram código, testam, e iteram. Ferramentas de IA como o Copilot Chat e CodeWhisperer fazem uso implícito de pedidos iterativos (ex: “Faça um teste para este código” / “Encontre erros”), que equivalem a supervisão em ciclo.

## Aprendizado por Currículo (Curriculum Learning)

**Descrição:** Currículo de aprendizado organiza o *treinamento* do modelo para que ele veja primeiro exemplos simples e depois gradualmente casos mais difíceis. Em vez de treinar LLMs de código diretamente em tarefas arbitrárias, o treinamento é dividido em estágios (“fácil”, “médio”, “difícil”) por complexidade do código. Na literatura, Naïr et al. (2024) aplicam isso a LLMs de código pequenos: eles definiram uma métrica de dificuldade de código, criaram conjuntos de dados graduais e treinaram modelos de 1M de parâmetros de forma incremental.

* **Propósito:** Facilitar o aprendizado de tarefas complexas de programação decompondo o desafio para o modelo. Por exemplo, começar com rotinas simples (soma de números, manipulação básica de strings) e só depois passar a algoritmos avançados ou problemas de *debugging*. Isto pode ajudar o modelo a consolidar conceitos básicos antes de lidar com casos complicados.
* **Funcionamento:** Divide-se o conjunto de treinamento em níveis de dificuldade (usando métricas como complexidade ciclomática, tamanho, aninhamento, etc.). O modelo treina primeiro em todos os exemplos “fáceis” por certo tempo; depois adiciona exemplos “médios” (mantendo os difíceis), e por fim os mais “difíceis”. Alternativamente, pode-se combinar exemplos de diferentes níveis em estágios (currículo híbrido). A ideia é um percurso ordenado do aprendizado.
* **Vantagens:** Estudos mostram que, para LLMs menores ou médios, um currículo bem projetado pode aumentar a acurácia em tarefas de execução de código. No caso citado, modelos treinados com currículo superaram baselines sem currículo em \~+5-10 pontos percentuais em acurácia de execução. Isso indica que o modelo internalizou conceitos básicos antes de enfrentar casos complexos. Em tese, para cenários de manutenção de código, o modelo poderia gradualmente absorver aspectos simples do projeto (como estilo ou padrões comuns) antes de assumir casos de uso avançados.
* **Limitações:** Criar um bom currículo requer métricas de dificuldade robustas, o que não é trivial para código (o que é “difícil”?). Além disso, nem sempre funciona: a mesma pesquisa nota que o impacto positivo foi menor em tarefas de conclusão de código (“next-token prediction”). Ou seja, o ganho pode ser específico para certos objetivos de treinamento. Em LLMs muito grandes ou com pré-treinamento extenso, o efeito do currículo tende a diminuir (pois o modelo já conhece muitas nuances). Por fim, adotar currículo torna o processo de treinamento mais complexo e longo, sem garantia de melhoria drástica em todos os cenários.

## Exemplos de Ferramentas e Sistemas Comerciais

* **GitHub Copilot:** Baseado no Codex da OpenAI, o Copilot utiliza *contexto local* do editor para sugerir código. Ele leva em conta não só o arquivo atual mas também arquivos abertos/tabs vizinhos e importações relacionadas. Por exemplo, ao ter um arquivo de testes e outro de implementação abertos simultaneamente, o Copilot infere contexto adicional e produz sugestões mais precisas. O Copilot está constantemente ajustando seus “prompts” internos para otimizar o uso do contexto local, mas não acessa diretamente todo o repositório. Em essência, ele aplica técnicas simples de RAG local (uso de arquivos abertos) e prompt design para maximizar a relevância do contexto.

* **Amazon CodeWhisperer:** Funciona de maneira similar, completando código baseado no que está no IDE (código e comentários). Uma diferença chave é que o CodeWhisperer foi treinado internamente com muito código AWS (e lê os comentários do código). Ele permite filtrar sugestões que reproduzem literalmente o treinamento (para evitar vazamentos de licença). Em termos de memória/contexto, o CodeWhisperer analisa trechos próximos e usa instruções em linguagem natural do desenvolvedor (comentários) para gerar código relevante.

* **Tabnine:** É um autocompletador AI configurável que roda localmente ou na nuvem. Por padrão, usa modelos treinados em repositórios públicos para sugerir trechos de código baseados em contexto local simples (tokens próximos). Porém, empresas podem treinar Tabnine em seus próprios repositórios privados (versão paga). Assim, Tabnine fornece “memória do projeto” implicitamente aprendendo os padrões do código da empresa. Ele não implementa RAG dinâmico, mas oferece contexto estendido armazenando vetores de código para rápida lookup, atuando como uma base de dados local de trechos frequentes.

* **Google Gemini Code Assist:** Projetado para ter consciência de contexto em larga escala. A versão Enterprise aceita conexão com repositórios privados e mantém uma janela de contexto gigantesca (\~1 milhão de tokens). Isso significa que o LLM interno vê praticamente todo o código relevante do projeto e pode referenciá-lo diretamente ao gerar sugestões. Na prática, o Gemini “local codebase awareness” é um RAG avançado: ele indexa o projeto inteiro (incluindo dependências) e usa esse repositório como fonte contextual para grounding. O usuário interage via chat e comandos “inteligentes” no IDE que acionam essas consultas internas, minimizando a necessidade de copiar arquivos manualmente.

Cada uma dessas ferramentas adota combinações de memória/contexto: Copilot e CodeWhisperer focam no contexto imediato de desenvolvimento e prompt tuning; Tabnine fornece “memória” via treinamento em código privado; Gemini cria um modelo híbrido com RAG pesado e janelas de contexto enormes. Em todos os casos, o objetivo é fazer com que o modelo tenha “mais conhecimento” do projeto além do prompt imediato, seja via prompts inteligentes, bases de dados externas, ou agentes múltiplos.

---

**Referências:** As informações acima foram baseadas em estudos recentes e documentações técnicas, ilustrando como diversos métodos – desde RAG até currículos de aprendizado – estão sendo explorados para enriquecer o contexto de agentes LLM de programação.
