# Projeto *Dia* (nari-labs/dia)

O **Dia** é um modelo de síntese de fala (*text-to-speech, TTS*) focado em diálogos desenvolvido pela Nari Labs. Trata-se de uma rede neural de **1,6 bilhão de parâmetros** capaz de gerar conversas realistas diretamente a partir de um roteiro escrito ([nari-labs/Dia-1.6B · Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B#:~:text=Dia%20is%20a%201,Hub%20using%20the%20PytorchModelHubMixin%20integration)). O modelo produz áudios com múltiplos falantes, controla emoções e entonação via *prompts* de áudio, e reconhece tags não-verbais (como `(laughs)`, `(coughs)`, etc.) embutidas no texto ([nari-labs/Dia-1.6B · Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B#:~:text=Dia%20is%20a%201,Hub%20using%20the%20PytorchModelHubMixin%20integration)). Seu código e pesos pré-treinados estão disponíveis publicamente: o repositório GitHub (licença Apache 2.0) fornece scripts Python e uma interface CLI, enquanto o modelo está hospedado no Hugging Face Hub ([nari-labs/Dia-1.6B · Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B#:~:text=Dia%20is%20a%201,Hub%20using%20the%20PytorchModelHubMixin%20integration)) ([GitHub - nari-labs/dia: A TTS model capable of generating ultra-realistic dialogue in one pass.](https://github.com/nari-labs/dia#:~:text=from%20dia)). Atualmente, Dia suporta apenas geração em inglês ([nari-labs/Dia-1.6B · Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B#:~:text=Dia%20directly%20generates%20highly%20realistic,laughter%2C%20coughing%2C%20clearing%20throat%2C%20etc)) e requer GPU potente (≈10 GB de VRAM) para rodar eficientemente ([dia/README.md at main · nari-labs/dia · GitHub](https://github.com/nari-labs/dia/blob/main/README.md#:~:text=Dia%20has%20been%20tested%20on,also%20needs%20to%20be%20downloaded)). Em suma, o projeto é uma iniciativa open-source de TTS de última geração voltada para diálogos expressivos, comparável a ferramentas comerciais como ElevenLabs (que não é open-source) ([A new, open source text-to-speech model called Dia has arrived to challenge ElevenLabs, OpenAI and more | VentureBeat](https://venturebeat.com/ai/a-new-open-source-text-to-speech-model-called-dia-has-arrived-to-challenge-elevenlabs-openai-and-more/#:~:text=In%20standard%20dialogue%20scenarios%2C%20Dia,output%20textual%20substitutions%20like%20%E2%80%9Chaha%E2%80%9D)).

## Notícias e atualizações recentes

O modelo Dia foi anunciado em **abril de 2025**, atraindo atenção na mídia de tecnologia. Um artigo do *VentureBeat* destacou que o Dia gera diálogos superiores a competidores como ElevenLabs Studio e o novo modelo *Sesame CSM-1B* (lançado pela Meta), especialmente em expressões não-verbais. Por exemplo, em um roteiro contendo `(laughs)`, Dia produz risada real no áudio, enquanto ElevenLabs somente inseriu “haha” no texto ([A new, open source text-to-speech model called Dia has arrived to challenge ElevenLabs, OpenAI and more | VentureBeat](https://venturebeat.com/ai/a-new-open-source-text-to-speech-model-called-dia-has-arrived-to-challenge-elevenlabs-openai-and-more/#:~:text=In%20standard%20dialogue%20scenarios%2C%20Dia,output%20textual%20substitutions%20like%20%E2%80%9Chaha%E2%80%9D)). O *VentureBeat* também observa que Dia é open-source (Apache 2.0) e roda sobre PyTorch 2.0 com CUDA 12.6, exigindo cerca de 10 GB de VRAM e entregando ~40 tokens/seg em GPU de classe empresarial ([A new, open source text-to-speech model called Dia has arrived to challenge ElevenLabs, OpenAI and more | VentureBeat](https://venturebeat.com/ai/a-new-open-source-text-to-speech-model-called-dia-has-arrived-to-challenge-elevenlabs-openai-and-more/#:~:text=Model%20access%20and%20tech%20specs)) ([dia/README.md at main · nari-labs/dia · GitHub](https://github.com/nari-labs/dia/blob/main/README.md#:~:text=Dia%20has%20been%20tested%20on,also%20needs%20to%20be%20downloaded)). A Nari Labs tem disponibilizado exemplos comparativos (no Notion e Space) e um *demo* Gradio; além disso, planeja versões futuras com suporte a CPU, quantização e até um produto para usuários finais (há até uma lista de espera por e-mail) ([A new, open source text-to-speech model called Dia has arrived to challenge ElevenLabs, OpenAI and more | VentureBeat](https://venturebeat.com/ai/a-new-open-source-text-to-speech-model-called-dia-has-arrived-to-challenge-elevenlabs-openai-and-more/#:~:text=Nari%20Labs%20is%20also%20developing,a%20waitlist%20for%20early%20access)). Outro indicativo do projeto é que a comunidade já criou pacotes auxiliares, como um *container* Cog que facilita a execução local do modelo ([GitHub - zsxkib/cog-dia: Cogified TTS model capable of generating ultra-realistic dialogue in one pass](https://github.com/zsxkib/cog-dia#:~:text=This%20repository%20provides%20a%20Cog,verbal%20sounds%20like%20%60%28laughs)). Em resumo, Dia é um projeto muito recente, com atualizações frequentes (no final de abril de 2025) e repercussão notável na comunidade de IA ([A new, open source text-to-speech model called Dia has arrived to challenge ElevenLabs, OpenAI and more | VentureBeat](https://venturebeat.com/ai/a-new-open-source-text-to-speech-model-called-dia-has-arrived-to-challenge-elevenlabs-openai-and-more/#:~:text=In%20standard%20dialogue%20scenarios%2C%20Dia,output%20textual%20substitutions%20like%20%E2%80%9Chaha%E2%80%9D)) ([GitHub - zsxkib/cog-dia: Cogified TTS model capable of generating ultra-realistic dialogue in one pass](https://github.com/zsxkib/cog-dia#:~:text=This%20repository%20provides%20a%20Cog,verbal%20sounds%20like%20%60%28laughs)).

## Como usar em Node.js

Para integrar o Dia numa aplicação Node.js, o caminho prático é usar a **API de inferência do Hugging Face** (que hospeda o modelo) via JavaScript. A biblioteca oficial `@huggingface/inference` facilita isso. Em resumo, proceda assim:

1. **Configurar o ambiente Node**: instale o Node.js e crie um projeto, por exemplo com `npm init`. Em seguida, instale o pacote de inferência do Hugging Face:  
   ```bash
   npm install @huggingface/inference
   ```  
   Este pacote permite chamar diretamente modelos hospedados no Hub via código JavaScript ([Inference Providers](https://huggingface.co/docs/hub/en/models-inference#:~:text=Yes%21%20We%20provide%20client%20wrappers,in%20both%20JS%20and%20Python)).

2. **Obter token de acesso**: crie uma conta no Hugging Face e gere um *Access Token* (permite chamadas à inferência). No código, use este token ao instanciar o cliente. Por exemplo, em Node:  
   ```js
   const { InferenceClient } = require('@huggingface/inference');
   const hf = new InferenceClient('SEU_TOKEN_AQUI');  // substitua pelo seu token
   ```  
   (O token **deve ser mantido em segredo**, idealmente carregado de uma variável de ambiente. Se a aplicação Node for exposta, considere usar um *proxy* para protegê-lo ([huggingface.js/packages/inference/README.md at main · huggingface/huggingface.js · GitHub](https://github.com/huggingface/huggingface.js/blob/main/packages/inference/README.md#:~:text=import%20,huggingface%2Finference)) ([huggingface.js/packages/inference/README.md at main · huggingface/huggingface.js · GitHub](https://github.com/huggingface/huggingface.js/blob/main/packages/inference/README.md#:~:text=Your%20access%20token%20should%20be,that%20stores%20the%20access%20token)).)

3. **Fazer requisição ao modelo**: use o método `textToSpeech` do cliente. Por exemplo:  
   ```js
   const texto = "[S1] Olá, tudo bem? [S2] Sim! Estou ótimo. (laughs)";
   const resposta = await hf.textToSpeech({
     model: 'nari-labs/Dia-1.6B',
     inputs: texto
   });
   ```  
   Isto envia o texto ao endpoint de inferência do Dia ([huggingface.js/packages/inference/README.md at main · huggingface/huggingface.js · GitHub](https://github.com/huggingface/huggingface.js/blob/main/packages/inference/README.md#:~:text=Generates%20natural,input)). (Alternativamente, você pode usar uma chamada `fetch` ao endpoint REST `https://api-inference.huggingface.co/models/nari-labs/Dia-1.6B` com método POST e cabeçalho de autorização ([Serverless Inference API - Hugging Face Open-Source AI Cookbook](https://huggingface.co/learn/cookbook/en/enterprise_hub_serverless_inference_api#:~:text=The%20Serverless%20Inference%20API%20exposes,Hub%20with%20a%20simple%20API)).)

4. **Salvar/usar o áudio**: a resposta (`resposta.data`) virá como áudio bruto (por padrão, em FLAC). Salve-o num arquivo ou processador de áudio. Por exemplo:  
   ```js
   const fs = require('fs');
   fs.writeFileSync('output.flac', Buffer.from(resposta.data));
   ```  
   Conforme indicado pela documentação Hugging Face, a API retorna um arquivo FLAC completo ([How to use the inference api on tts model? - Models - Hugging Face Forums](https://discuss.huggingface.co/t/how-to-use-the-inference-api-on-tts-model/12397#:~:text=At%20this%20point%20you%20should,you%20can%20just%20save%20it)). Você pode então converter ou reproduzir este FLAC (p. ex. usando `ffmpeg` ou bibliotecas de áudio). 

5. **Testar e ajustar**: verifique o áudio gerado e ajuste o texto de entrada ou use *audio prompts* (clips de voz) para controlar o estilo/voz. O modelo tende a gerar vozes diferentes a cada execução, a menos que você fixe a semente de aleatoriedade ou forneça um prompt de áudio guia ([A new, open source text-to-speech model called Dia has arrived to challenge ElevenLabs, OpenAI and more | VentureBeat](https://venturebeat.com/ai/a-new-open-source-text-to-speech-model-called-dia-has-arrived-to-challenge-elevenlabs-openai-and-more/#:~:text=The%20model%20is%20currently%20English,by%20uploading%20a%20sample%20clip)). Não se esqueça de alternar as tags `[S1]` e `[S2]` entre falantes no texto, conforme exigido pelo modelo ([nari-labs/Dia-1.6B · Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B#:~:text=Dia%20is%20a%201,Hub%20using%20the%20PytorchModelHubMixin%20integration)).

Em resumo, você pode implementar Dia em Node.js instalando `@huggingface/inference`, autenticando com seu token HF e chamando `hf.textToSpeech({model: 'nari-labs/Dia-1.6B', inputs: '...'}).save()` para obter o áudio ([huggingface.js/packages/inference/README.md at main · huggingface/huggingface.js · GitHub](https://github.com/huggingface/huggingface.js/blob/main/packages/inference/README.md#:~:text=Generates%20natural,input)) ([huggingface.js/packages/inference/README.md at main · huggingface/huggingface.js · GitHub](https://github.com/huggingface/huggingface.js/blob/main/packages/inference/README.md#:~:text=import%20,huggingface%2Finference)).

## Como usar em React.js

Para usar Dia numa aplicação web React.js, a abordagem é semelhante: utilizar a API do Hugging Face por requisições HTTP. Por exemplo: 

1. **Criar app React**: use `npx create-react-app` (ou outra ferramenta) para iniciar. Armazene seu token HF em `process.env.REACT_APP_HF_TOKEN` (ou num arquivo `.env`) para não expor no código.  

2. **Chamar a API com *fetch* ou Axios**: em algum componente, faça uma requisição POST ao endpoint do modelo. Por exemplo:  
   ```js
   const url = 'https://api-inference.huggingface.co/models/nari-labs/Dia-1.6B';
   const payload = { inputs: "[S1] Olá! [S2] Como vai?" };
   const headers = { 
     'Authorization': `Bearer ${process.env.REACT_APP_HF_TOKEN}`,
     'Content-Type': 'application/json'
   };
   fetch(url, { method: 'POST', headers, body: JSON.stringify(payload) })
     .then(res => res.blob())
     .then(blob => {
       const audioUrl = URL.createObjectURL(blob);
       // Defina este audioUrl numa tag <audio> para reprodução:
       document.getElementById('player').src = audioUrl;
     });
   ```  
   Isso envia o texto ao Hugging Face e recebe um *blob* FLAC como resposta. Em React, você pode então usar uma tag `<audio id="player" controls>` para reproduzir o áudio gerado. (A API retorna FLAC, conforme o fórum indica ([How to use the inference api on tts model? - Models - Hugging Face Forums](https://discuss.huggingface.co/t/how-to-use-the-inference-api-on-tts-model/12397#:~:text=At%20this%20point%20you%20should,you%20can%20just%20save%20it)) – o navegador moderno geralmente reproduz FLAC ou você pode convertê-lo para MP3/OGG usando bibliotecas do lado cliente.)

3. **Cuidado com o token**: não inclua diretamente o token HF no código cliente em produção, pois ele ficaria exposto. Prefira chamar um servidor intermediário (backend) que armazene o token, ou use funções serverless para fazer essa ponte ([huggingface.js/packages/inference/README.md at main · huggingface/huggingface.js · GitHub](https://github.com/huggingface/huggingface.js/blob/main/packages/inference/README.md#:~:text=import%20,huggingface%2Finference)) ([huggingface.js/packages/inference/README.md at main · huggingface/huggingface.js · GitHub](https://github.com/huggingface/huggingface.js/blob/main/packages/inference/README.md#:~:text=Your%20access%20token%20should%20be,that%20stores%20the%20access%20token)). O client React em si pode chamar seu backend, que por sua vez acessa a API do Hugging Face. 

4. **Exibir resultados**: após a geração do áudio, insira-o na interface. Por exemplo, renderize um player de áudio HTML para o usuário ouvir o resultado. Você também pode mostrar logs de texto ou indicadores de loading enquanto a geração é processada. 

Em suma, em React.js você trata Dia como qualquer serviço web: configure o ambiente, faça requisição HTTP ao endpoint do modelo (`api-inference.huggingface.co/models/nari-labs/Dia-1.6B`) com seu texto e token, e reproduza o áudio retornado no navegador ([Serverless Inference API - Hugging Face Open-Source AI Cookbook](https://huggingface.co/learn/cookbook/en/enterprise_hub_serverless_inference_api#:~:text=The%20Serverless%20Inference%20API%20exposes,Hub%20with%20a%20simple%20API)) ([How to use the inference api on tts model? - Models - Hugging Face Forums](https://discuss.huggingface.co/t/how-to-use-the-inference-api-on-tts-model/12397#:~:text=At%20this%20point%20you%20should,you%20can%20just%20save%20it)).

## Exemplos de uso e tutoriais

- **Script de clonagem de voz**: o repositório inclui um exemplo em Python (`example/voice_clone.py`) que mostra como usar um *prompt* de áudio para imitar a voz de uma pessoa. Veja o código em [example/voice_clone.py](https://github.com/nari-labs/dia/blob/main/example/voice_clone.py) ([GitHub - nari-labs/dia: A TTS model capable of generating ultra-realistic dialogue in one pass.](https://github.com/nari-labs/dia#:~:text=,the%20content%20of%20your%20script)). No *Hugging Face Space* (demo web) é possível fazer upload de um áudio guia conforme descrito ali.  
- **Demos interativos**: existe um *Hugging Face Space* oficial onde você pode testar o modelo sem código: [Spaces – *Dia 1.6B* (nari-labs/Dia-1.6B)](https://huggingface.co/spaces/nari-labs/Dia-1.6B). Esse espaço roda em CPU (ZeroGPU) mas permite digitar scripts e obter áudio gerado ([nari-labs/Dia-1.6B · Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B#:~:text=,and%20access%20to%20new%20features)). Também há uma página de demonstração comparativa (Notion) ligada no repositório.  
- **Conteúdo da comunidade**: além do artigo do VentureBeat, surgiram posts em redes como Reddit e vídeos no YouTube comparando o Dia a outras ferramentas (por exemplo, *"Nari DIA 1.6B TTS Better than ElevenLabs"*). Esses materiais mostram exemplos práticos de uso e podem servir de guia adicional.  
- **Recursos oficiais**: consulte sempre o **README do GitHub** do projeto (contém instruções e links úteis) ([GitHub - nari-labs/dia: A TTS model capable of generating ultra-realistic dialogue in one pass.](https://github.com/nari-labs/dia#:~:text=from%20dia)) ([dia/README.md at main · nari-labs/dia · GitHub](https://github.com/nari-labs/dia/blob/main/README.md#:~:text=Dia%20is%20a%201,model%20created%20by%20Nari%20Labs)). O GitHub do projeto [nari-labs/dia](https://github.com/nari-labs/dia) concentra documentação e exemplos de código. A página do modelo no Hugging Face Hub também traz trechos de código de uso e informações (como este resumo: *“text to speech model created by Nari Labs”* ([nari-labs/Dia-1.6B · Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B#:~:text=Dia%20is%20a%201,Hub%20using%20the%20PytorchModelHubMixin%20integration))). 
- **Tabela de referências**: para rápida consulta, seguem alguns links importantes:
  
  | Recurso                    | Link                                      |
  |----------------------------|-------------------------------------------|
  | GitHub *nari-labs/dia*     | https://github.com/nari-labs/dia          |
  | Hugging Face (modelo Dia)  | https://huggingface.co/nari-labs/Dia-1.6B |
  | Hugging Face Space         | https://huggingface.co/spaces/nari-labs/Dia-1.6B |
  | Artigo no VentureBeat      | *ver referências abaixo*                  |
  | Exemplo *voice_clone.py*   | [blob/main/example/voice_clone.py](https://github.com/nari-labs/dia/blob/main/example/voice_clone.py) |
  | Cog container (terceiros)  | https://github.com/zsxkib/cog-dia         |

Todos os detalhes técnicos e atualizações mais recentes podem ser encontrados nesses recursos oficiais e nas citações acima, garantindo um bom ponto de partida para implementar e experimentar o Dia em aplicações Node.js e React.js ([nari-labs/Dia-1.6B · Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B#:~:text=Dia%20is%20a%201,Hub%20using%20the%20PytorchModelHubMixin%20integration)) ([dia/README.md at main · nari-labs/dia · GitHub](https://github.com/nari-labs/dia/blob/main/README.md#:~:text=Dia%20has%20been%20tested%20on,also%20needs%20to%20be%20downloaded)) ([huggingface.js/packages/inference/README.md at main · huggingface/huggingface.js · GitHub](https://github.com/huggingface/huggingface.js/blob/main/packages/inference/README.md#:~:text=import%20,huggingface%2Finference)) ([Serverless Inference API - Hugging Face Open-Source AI Cookbook](https://huggingface.co/learn/cookbook/en/enterprise_hub_serverless_inference_api#:~:text=The%20Serverless%20Inference%20API%20exposes,Hub%20with%20a%20simple%20API)).

**Fontes:** Documentação oficial do projeto (GitHub, Hugging Face) e artigos/conteúdos confiáveis (por exemplo, VentureBeat) foram usados para compilar este relatório ([nari-labs/Dia-1.6B · Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B#:~:text=Dia%20is%20a%201,Hub%20using%20the%20PytorchModelHubMixin%20integration)) ([A new, open source text-to-speech model called Dia has arrived to challenge ElevenLabs, OpenAI and more | VentureBeat](https://venturebeat.com/ai/a-new-open-source-text-to-speech-model-called-dia-has-arrived-to-challenge-elevenlabs-openai-and-more/#:~:text=In%20standard%20dialogue%20scenarios%2C%20Dia,output%20textual%20substitutions%20like%20%E2%80%9Chaha%E2%80%9D)) ([dia/README.md at main · nari-labs/dia · GitHub](https://github.com/nari-labs/dia/blob/main/README.md#:~:text=Dia%20has%20been%20tested%20on,also%20needs%20to%20be%20downloaded)) ([huggingface.js/packages/inference/README.md at main · huggingface/huggingface.js · GitHub](https://github.com/huggingface/huggingface.js/blob/main/packages/inference/README.md#:~:text=import%20,huggingface%2Finference)). As instruções de uso em Node.js e React.js baseiam-se nas APIs do Hugging Face para TTS ([huggingface.js/packages/inference/README.md at main · huggingface/huggingface.js · GitHub](https://github.com/huggingface/huggingface.js/blob/main/packages/inference/README.md#:~:text=Generates%20natural,input)) ([How to use the inference api on tts model? - Models - Hugging Face Forums](https://discuss.huggingface.co/t/how-to-use-the-inference-api-on-tts-model/12397#:~:text=At%20this%20point%20you%20should,you%20can%20just%20save%20it)) ([Serverless Inference API - Hugging Face Open-Source AI Cookbook](https://huggingface.co/learn/cookbook/en/enterprise_hub_serverless_inference_api#:~:text=The%20Serverless%20Inference%20API%20exposes,Hub%20with%20a%20simple%20API)).