Ótimo, vou pesquisar APIs comerciais e open source que permitem separar e identificar vozes diferentes em arquivos de áudio, tanto para áudio pré-gravado quanto em tempo real. Também incluirei opções que apenas separam os falantes, caso o reconhecimento não esteja disponível.

Vou te mostrar soluções que funcionam bem para reuniões, podcasts e outros tipos de gravações com múltiplas vozes.

# APIs Comerciais de Diarização e Reconhecimento de Falantes

As grandes plataformas de nuvem e startups especializadas oferecem APIs que realizam *diarização de falantes* (segmentação de áudio por locutor) e, em alguns casos, identificação dos mesmos. A seguir comparamos as principais soluções comerciais:

| Serviço                  | Áudio em Tempo Real | Arquivos Gravados | Diarização de Locutores | Reconhecimento (ID) | Idiomas Suportados       | Modelo de Preço (link)         | Observações (qualidade/limites)                                    |
|--------------------------|---------------------|-------------------|-------------------------|---------------------|--------------------------|-------------------------------|-------------------------------------------------------------------|
| *Google Cloud Speech-to-Text*  | Sim (streaming gRPC/WebSocket)  | Sim               | Sim (ativa via enableSpeakerDiarization) | Não (somente rotula genérico “speaker 1, 2, ...”) | 120+ idiomas (incl. PT-BR) | Pago por minuto (p.ex. $0.006/15s em streaming) | Suporta diarização de múltiplos canais (cada canal um locutor) – até ~6 falantes por canal; boa precisão em áudio claro, tende a confundir com ruído/background. Integração via SDKs (Python, Node, etc) bem documentada. |
| *Amazon Transcribe (AWS)*    | Sim (streaming/Kinesis)           | Sim               | Sim (speaker labels)    | Parcial (rotula falantes mas sem nome) | Inglês, Espanhol, Francês, Português (BR), Alemão, etc. ~9 idiomas | Pago por segundo de áudio (ex: ~$0.0008/s) | Até 10 falantes identificáveis. Suporta canais múltiplos. Qualidade sólida em áudio de reunião, mas menos eficaz em ambientes muito ruidosos. Bom SDK/API REST; documentação robusta. |
| *Azure Speech (Microsoft)*   | Sim (conversação em tempo real)  | Sim               | Sim (diarização via Conversation Transcription) | Sim (pode vincular locutores conhecidos com Speaker Recognition) | ~10 idiomas + dialetos (inclui PT-BR) | Pago por hora de transcrição (ex: $1.00/h) | Oferece transcrição de conversas com reconhecimento de locutores (voice profiling). Limitado a ~10 locutores simultâneos. Alta acurácia em audiências claras; SDKs e exemplos em C#, Python, JS. |
| *IBM Watson Speech-to-Text*  | Sim (websocket)      | Sim               | Sim (“speaker_labels”)  | Não (apenas marcas genéricas)         | 8+ idiomas (incl. Português do Brasil) | Plano gratuito limitado; pago por minuto (ex: $0.02/min) | Pode sinalizar até 6 locutores distintos. Bom para idiomas disponíveis, mas taxa de erro maior em sotaques fortes ou baixo SNR. Ferramenta “Speaker Labels” requer ajuste; documentação completa. |
| *AssemblyAI*              | Sim (WebSocket)     | Sim               | Sim (auto ativa)        | Não nativo (foco em identificação por voz pós-processamento) | Inglês (principal); alguns modelos limitados em Espanhol | $0.015/min para transcrição (US$0.021/min para diarização) | API especializada em transcrição avançada. Suporta até 10 locutores; facilita resumir podcasts/reuniões. É muito precisa mesmo com barulho moderado. Boa documentação e SDK em Python. |
| *Deepgram*                | Sim (gRPC)          | Sim               | Sim (configurável)      | Não (não identifica pelo nome)        | Inglês, Espanhol, Alemão, Francês, Português (BR), etc. | Planos mensais; grátis 45 min/mês, depois pago por hora (ex: $4.50/h) | Usa modelos neurais end-to-end; robusto a ruído e música de fundo. Limita-se a ~10 falantes. Oferece API simples com SDKs (Node, Python) e boa documentação. |
| *Speechmatics*            | Sim (Real-time API) | Sim               | Sim (pode diferenciar locutores) | Não (somente tags de locutor)         | >30 idiomas (inclui PT-BR) | Por hora de áudio (ex: £0.03/min) | Preciso em vários idiomas; lida bem com sotaques. Identifica até ~10 falantes. Tem API REST e bibliotecas. Pode requerer pós-processamento para ruído intenso. |
| *Rev.ai*                  | Não (somente batch) | Sim               | Sim (opção no pedido)   | Não (só rótulos de falante)         | Inglês (primário) | $0.035/min (transcrição básica) | Fornece transcrição com rotulação de falantes. Alto grau de acerto no inglês, mas suporte limitado de idiomas. API fácil de usar. Não suporta streaming em tempo real. |

Para todas as APIs acima, a *diarização* consiste em segmentar o áudio por falante sem necessariamente identificar quem é quem (os locutores são rotulados genericamente). O *reconhecimento de locutor* (vincular um nome ou ID ao falante) é raro em soluções cloud atuais – exceto em cenários avançados do Azure, por exemplo, que permite *vincular perfis pré-criados* aos segmentos reconhecidos. Os preços variam: em geral cobram por minuto de áudio processado (e.g. Google e AWS ~ US$0.006/15s, Azure ~$1/h, etc.). Todas oferecem SDKs ou bibliotecas oficiais (Python, JavaScript, etc.) e documentação detalhada. Em termos de limitações, é importante notar o *número máximo de falantes* suportados (geralmente ~6–10), a qualidade do áudio (ruído e eco podem degradar a separação) e o *idioma* – nem todas suportam todas as línguas (por exemplo, alguns serviços cobrem bem o inglês mas têm menos opções para português).

## APIs Open Source de Diarização

Para projetos sem custo de licenciamento, existem bibliotecas e ferramentas open-source:

- *pyannote.audio* : Biblioteca Python especializada em diarização de fala. Possui modelos pré-treinados capazes de segmentar múltiplos falantes em arquivo de áudio. Funciona apenas *offline* (não projetada para streaming em tempo real), suporta qualquer idioma pois baseia-se em características acústicas. É complexa de ajustar, mas é referência acadêmica (qualidade muito alta em cenários limpos). Licença livre (MIT); documentação técnica disponível. Limitações: requer bom GPU, processamento lento; não faz reconhecimento de identidade sem treino adicional de voiceprints.
- *SpeechBrain* : Framework de áudio que inclui pipelines de diarização e reconhecimento de locutor (speaker verification). Oferece modelo pré-treinado para separar falantes em gravações. Suporta áudio gravado, mas não streaming real-time nativamente. Multilíngue (modelos genéricos), gratuito e com tutoriais. A qualidade é comparável ao estado-da-arte em ambientes controlados. Como desvantagem, precisa gerenciar configuração de modelos e inferência.
- *LIUM SpkDiarization* (antigo): Toolkit em Java gratuito para diarização. Desenvolvido em pesquisas acadêmicas francesas; pode segmentar locutores em arquivos de áudio. Já não é mantido ativamente, mas ainda funciona para provas de conceito. Suporta somente arquivos pré-gravados e uma quantidade limitada de falantes. A documentação é básica (originais de 2010).
- *Kaldi (com scripts de diarização)*: Kaldi é um toolkit amplamente usado em pesquisa de ASR e possui recipes para diarização de locutores usando clustering de embeddings. Não é um serviço simples, mas oferece alto controle sobre modelos. Requer considerável expertise para configurar. Gratuito; multi-idioma (depende de modelos de rede neural usados). Pode ser combinado com kaldi de transcrição para pipeline completo.
- *Outros: Projetos menores como Resemblyzer (focado em embeddings de voz, útil para comparar similares) ou até combinar segmentação de silêncio (pyannote/WebRTC VAD) com verificação de locutor podem ser usados artesanalmente. **Whisper* da OpenAI (open-source) oferece transcrição offline de alta qualidade e, em combinação com algoritmos de segmentação, pode ser usado para diarizar, mas por si só não separa falantes ou faz identificação.

### Comparação Resumida

| Solução                | Tipo             | Tempo Real | Áudio Gravado | Reconhecimento de Identidade | Idiomas Suportados     | Preço   | Complexidade / Observações                                         |
|------------------------|------------------|------------|---------------|------------------------------|------------------------|---------|---------------------------------------------------------------------|
| *pyannote.audio*     | Biblioteca OSS   | Não        | Sim           | Não (só clusters)            | Indiferente ao idioma  | Gratuito | SOTA em diarização offline; requer GPU; sem suporte nativo a streaming. |
| *SpeechBrain*        | Biblioteca OSS   | Não        | Sim           | Parcial (verificação)        | Vários (modelos pré)   | Gratuito | Fácil uso relativo; suporta speaker embedding para reconhecimento. |
| *LIUM SpkDiarization*| Aplicação OSS    | Não        | Sim           | Não                          | Qualquer (base acústica)| Gratuito | Antigo mas funcional; sem manutenção recente.               |
| *Kaldi*              | Toolkit OSS      | Não        | Sim           | Parcial (depende de receitas)| Multi (depende do AM)  | Gratuito | Extensível e poderoso, porém complexo para leigos.                   |
| *OpenAI Whisper*     | Modelo OSS       | Não        | Sim           | Não                          | 99 idiomas de transcrição | Gratuito | Excelente transcrição; diarização manual via pós-processo; offline. |

Cada solução open-source exige mais trabalho para integrar do que uma API paga: não há SDK “plug-and-play” com suporte técnico. Em compensação, são gratuitas e podem ser treinadas ou adaptadas. A *diarização open-source* costuma atingir alta precisão em áudio limpo e com poucas interferências, mas pode ter dificuldades em tempo real ou em ambientes ruidosos. 

*Fontes:* Documentação oficial das APIs mencionadas e referências técnicas das bibliotecas open-source (consultadas até 2025).