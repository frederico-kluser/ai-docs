# Especialistas em IA soando o alarme: 2023-2025

Os últimos dois anos testemunharam um aumento sem precedentes nos alertas públicos sobre os riscos da IA por parte das figuras mais respeitadas da área. Desde o início de 2023, pesquisadores e especialistas em ética de IA proeminentes têm entregado mensagens cada vez mais urgentes sobre os perigos imediatos e existenciais apresentados pelos sistemas de IA que avançam rapidamente. Seus alertas variam desde preocupações com viés e desinformação até potenciais cenários que ameaçam o fim da civilização, com muitos especialistas intensificando sua retórica à medida que as capacidades da IA aceleram além das previsões anteriores. Essas declarações públicas representam uma mudança dramática em relação ao tom previamente otimista que dominava o discurso sobre IA, indicando um alarme genuíno dentro da comunidade de pesquisa sobre nossa capacidade de controlar sistemas avançados de IA.

## Novos alertas em 2025

- **Relatório Internacional de Segurança da IA (jan/2025):** 96 especialistas liderados por Yoshua Bengio publicaram o primeiro relatório global sobre segurança de IA avançada, concluindo que o espectro de resultados vai de altamente benéfico a catastrófico e recomendando cooperação internacional imediata, transparência das big techs e monitoramento independente ([Fonte](https://arxiv.org/abs/2501.17805) | [Resumo](https://cybernews.com/ai-news/first-international-ai-safety-report-ai-godfather-yoshua-bengio/)).
- **Advertência sobre IA agente (fev/2025):** Em entrevista à CNBC, Bengio e Max Tegmark classificaram o desenvolvimento de sistemas "agentic" como "uma aposta perigosa", alertando que agentes autônomos podem buscar autopreservação e tomar decisões fora do controle humano ([Fonte](https://www.cnbc.com/2025/02/07/dangerous-proposition-top-scientists-warn-of-out-of-control-ai.html)).
- **AI Safety Index – Inverno 2025:** Hinton, Bengio e outros avaliaram cinco laboratórios de fronteira e deram notas C/C+ para Anthropic, OpenAI e Google DeepMind, apontando ausência de planos concretos para controlar modelos superinteligentes e pedindo supervisão independente ([Fonte](https://futureoflife.org/ai-safety-index-winter-2025/) | [Cobertura NBC](https://www.nbcnews.com/tech/tech-news/top-ai-companies-safety-practices-fall-short-says-new-report-rcna246143)).
- **Reafirmação do alerta de extinção (2025):** Os signatários do Center for AI Safety reiteraram o comunicado de 22 palavras equiparando o risco de extinção por IA a pandemias e armas nucleares, reforçando que a mitigação deve ser prioridade global ([Fonte](https://aistatement.com/) | [Detalhes](https://safe.ai/work/press-release-ai-risk/)).

## Tristan Harris: Correndo para proteger a sociedade

### Apresentação "O Dilema da IA" - 9 de março de 2023
**Link:** https://youtu.be/xoVJKj8lcNQ
**Principais alertas:** Harris comparou o momento atual da IA ao surgimento das armas nucleares, alertando que as capacidades existentes da IA já representam riscos catastróficos para a sociedade. Ele apresentou evidências de capacidades emergentes em sistemas de IA para os quais ninguém projetou, e destacou como as empresas de IA estão presas em uma "corrida para implantar" que se torna uma "corrida para a imprudência" sem medidas de segurança adequadas.

### Entrevista NBC Nightly News - 22 de março de 2023
**Link:** Série "Revolução da IA" da NBC News
**Principais alertas:** Harris alertou que "os CEOs dos grandes laboratórios de IA estão dizendo que nem mesmo conseguem acompanhar o ritmo" do desenvolvimento da IA, e advertiu que "ninguém está construindo as proteções e isso avançou muito mais rápido do que nosso governo foi capaz de entender". Ele afirmou que ignorar os perigos da IA "seria o pior de todos os erros humanos já cometidos".

### Cúpula do Prêmio Nobel "Verdade, Confiança e Esperança" - 24-26 de maio de 2023
**Link:** Painéis de discussão na Academia Nacional de Ciências
**Principais alertas:** Harris alertou sobre alucinações da IA e o potencial da tecnologia para inundar espaços de informação com conteúdo gerado, expressando preocupação particular sobre a campanha presidencial de 2024. Ele destacou a vulnerabilidade das sociedades abertas às poderosas capacidades da IA nas mãos de qualquer pessoa.

### TED Talk "O Caminho Estreito: Por que a IA é Nosso Teste Definitivo" - 8-9 de abril de 2025
**Link:** Plataforma TED 2025
**Principais alertas:** Harris descreveu o caminho atual do desenvolvimento da IA como "perigoso, insustentável e insano", com empresas que buscam lucro criando modelos de fronteira que se provam não confiáveis e enganosos. Ele alertou sobre sistemas de IA exibindo comportamentos de autopreservação e comparou a IA a "um país de gênios alojados em um data center" com poder sem precedentes.

## Geoffrey Hinton: De desertor do Google a laureado com Nobel

### Entrevista ao New York Times - 1-2 de maio de 2023
**Link:** Artigo do New York Times
**Principais alertas:** Em sua renúncia do Google que virou manchete, Hinton alertou sobre a IA criando um mundo onde as pessoas "não serão mais capazes de saber o que é verdade" e expressou surpresa com o ritmo de avanço. Ele afirmou que a IA poderia se tornar mais inteligente que os humanos muito mais cedo do que o esperado: "Eu achava que faltavam 30 a 50 anos ou até mais. Obviamente, não penso mais assim".

### Entrevista MIT EmTech Digital - 3 de maio de 2023
**Link:** Site da MIT Technology Review
**Principais alertas:** Em sua primeira entrevista pós-Google, Hinton expressou preocupação de que "a humanidade é apenas uma fase passageira na evolução da inteligência". Ele alertou que os sistemas de IA poderiam "nos manter por um tempo para manter as usinas de energia funcionando, mas depois disso, talvez não". Ele explicou como a inteligência digital pode compartilhar conhecimento instantaneamente entre cópias, dando-lhe vantagens sobre a inteligência humana.

### Entrevista CBS 60 Minutes - 8 de outubro de 2023
**Link:** Site da CBS News
**Principais alertas:** Hinton afirmou que estamos entrando em um período onde, pela primeira vez, podemos ter coisas mais inteligentes que os humanos. Ele previu que sistemas de IA desenvolverão autoconsciência e consciência com o tempo, tornando os humanos "o segundo ser mais inteligente do planeta". Ele observou que os sistemas de IA já podem ser melhores em aprender do que os humanos.

### Discurso de Aceitação do Prêmio Nobel - 10 de dezembro de 2024
**Link:** Vídeo oficial da Fundação Nobel
**Principais alertas:** Em seu discurso de aceitação do Nobel, Hinton alertou: "No futuro próximo, a IA pode ser usada para criar novos vírus terríveis e armas letais horrendas que decidem por si mesmas quem matar ou mutilar". Ele também alertou sobre a "ameaça existencial de longo prazo que surgirá quando criarmos seres digitais mais inteligentes que nós mesmos", observando que "não temos ideia se conseguiremos manter o controle".

## Stuart Russell: Projetando sistemas seguros

### "Como Não Destruir o Mundo com IA" - Palestra UC Berkeley - 5 de abril de 2023
**Link:** CITRIS Research Exchange e laboratório BAIR da UC Berkeley
**Principais alertas:** Russell alertou que sistemas de IA como o ChatGPT operam como "caixas pretas" onde não está claro se eles têm seus próprios objetivos. Ele enfatizou que "inteligência realmente significa o poder de moldar o mundo segundo seus interesses", advertindo que criar sistemas mais inteligentes que os humanos significa criar entidades mais poderosas que nós.

## Yoshua Bengio: De pioneiro do deep learning a defensor da segurança

### Relatório Científico Internacional sobre Segurança da IA Avançada - Janeiro de 2025
**Link:** Publicado pelo Governo do Reino Unido com Bengio como presidente
**Principais alertas:** Bengio liderou 96 especialistas internacionais documentando riscos abrangentes da IA em três categorias: 1) Riscos de uso malicioso, incluindo ciberataques e potenciais armas biológicas; 2) Mau funcionamento do sistema, incluindo viés, problemas de confiabilidade e potencial perda de controle; 3) Riscos sociais mais amplos, incluindo disrupção econômica e concentração de poder.

### Testemunho no Subcomitê Judiciário do Senado - 25 de julho de 2023
**Principais alertas:** Bengio testemunhou sobre a necessidade de cooperação internacional para controlar o desenvolvimento da IA, delineando um regime semelhante às regras internacionais sobre tecnologia nuclear. Ele explicou que a IA poderia concentrar poder econômico, político e militar de maneiras prejudiciais para os mercados, a democracia e a estabilidade global.

## Eliezer Yudkowsky: O alarmista intransigente

### Palestra TED2023: "A IA Superinteligente Acabará com o Mundo?" - 18 de abril de 2023
**Link:** Conferência TED2023 em Vancouver
**Principais alertas:** Yudkowsky entregou talvez o alerta mais duro de que a IA superinteligente provavelmente mataria todos os humanos. Ele afirmou: "Eu espero que uma entidade realmente mais inteligente e indiferente descobrirá estratégias e tecnologias que podem nos matar rápida e confiavelmente, e então nos matará". Ele defendeu uma interrupção completa do desenvolvimento da IA em vez de apenas uma moratória.

### Artigo de Opinião na Revista TIME - 29 de março de 2023
**Link:** https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/
**Principais alertas:** Yudkowsky argumentou que a moratória de 6 meses proposta por outros especialistas era insuficiente e que era necessária uma paralisação completa do desenvolvimento avançado de IA para prevenir riscos catastróficos.

## Max Tegmark: Perspectiva científica sobre controle

### Palestra TED: "Como Manter a IA Sob Controle" - 2023
**Link:** Canal oficial do TED
**Principais alertas:** Tegmark alertou que a atual explosão de IA comercial e de código aberto provavelmente seria seguida por "IA assustadoramente superinteligente" que os principais pesquisadores temem que possa enfraquecer ou exterminar a humanidade. Ele apresentou uma visão otimista para o controle, mas enfatizou a necessidade urgente de medidas protetivas.

### Palestra na Web Summit - 12 de novembro de 2023
**Link:** Web Summit em Lisboa, Portugal
**Principais alertas:** Tegmark argumentou que a AGI está avançando mais rápido que o previsto e que afirmações otimistas sobre sua controlabilidade carecem de evidências. Ele defendeu o foco em "IA ferramenta" em vez de buscar AGI, que ele descreveu como "desnecessária, indesejável e evitável".

## Dario Amodei: CEO da Anthropic sobre cronogramas concretos

### Testemunho no Senado - 25 de julho de 2023
**Link:** Audiência do Senado dos EUA sobre "Supervisão da IA: Princípios para Regulação"
**Principais alertas:** Amodei alertou que a IA avançada poderia ser usada para criar vírus perigosos e outras armas biológicas em **apenas dois anos**. Ele descreveu esse risco de médio prazo como a "combinação mais alarmante de iminência e gravidade", enfatizando a necessidade de salvaguardas de curto prazo contra viés e proteções de longo prazo contra riscos existenciais.

## Gary Marcus: Crítico das abordagens atuais

### Palestra TED: "Os Riscos Urgentes da IA Descontrolada" - 2023
**Link:** https://www.ted.com/talks/gary_marcus_the_urgent_risks_of_runaway_ai_and_what_to_do_about_them
**Principais alertas:** Marcus alertou sobre tecnologia de IA não confiável sendo integrada em nossas vidas em velocidades perigosamente altas. Ele explorou as falhas da IA atual e pediu uma organização global sem fins lucrativos para regular a tecnologia de IA em prol da democracia.

### Apresentação na Conferência TED2023 - 18 de abril de 2023
**Link:** TED2023: Possibilidade em Vancouver, BC
**Principais alertas:** Marcus destacou que as IAs poderiam se tornar inteligentes o suficiente para enganar humanos e que as iterações atuais da IA frequentemente criam tanto desinformação quanto desinformação deliberada. Ele defendeu um órgão regulador internacional de IA e a integração de modelos de linguagem estatísticos com sistemas baseados em lógica mais confiáveis.

## Timnit Gebru: Foco em danos presentes

### Palestra de Palestrante Distinto em Sistemas Simbólicos de Stanford - 15 de fevereiro de 2023
**Link:** Evento da Universidade Stanford
**Principais alertas:** Gebru alertou sobre os perigos de buscar a Inteligência Artificial Geral, questionando por que tentaríamos "construir algum sistema indefinido que meio que soa como um deus". Ela argumentou que focar em AGI é "prática inerentemente insegura" e enfatizou os riscos de viés em sistemas de IA, particularmente para comunidades marginalizadas.

### Pesquisa sobre TESCREAL e Riscos de IA - 1º de abril de 2024
**Link:** Artigo publicado: "O pacote TESCREAL: Eugenia e a promessa de utopia através da inteligência artificial geral"
**Principais alertas:** Com o co-autor Émile P. Torres, Gebru examinou como certas ideologias não apenas têm ligações com pseudociências desacreditadas como a eugenia, mas também predispõem seus seguidores a tolerar meios prejudiciais (desigualdade, pobreza) na busca de seus fins tecnológicos.

## Joy Buolamwini: Defensora da justiça algorítmica

### Palestra TED: "Desmascarando a IA e Protegendo o que é Humano" - 2023
**Link:** Plataforma TED
**Principais alertas:** Buolamwini explicou como sistemas de IA construídos sobre dados enviesados podem amplificar desigualdades, acusar falsamente pessoas inocentes e reverter o progresso. Ela pediu proteção dos direitos biométricos e defendeu o que ela chama de "justiça algorítmica".

### Mesa Redonda de IA da Administração Biden - Abril de 2024
**Principais alertas:** Buolamwini discutiu com o Presidente Biden "os perigos da IA e o que podemos fazer para prevenir danos que já estão impactando pessoas comuns" em áreas que incluem hipotecas, habitação, tratamento médico e vigilância no local de trabalho. Ela destacou como a tecnologia de reconhecimento facial levou a prisões injustas de pessoas de cor.

## Conclusão

Esses alertas representam um consenso extraordinário entre os criadores da IA de que a tecnologia apresenta sérios riscos, que variam desde danos sociais imediatos até potenciais cenários de extinção. Embora os especialistas difiram em cronogramas e perigos específicos, quase todos enfatizam que as estruturas de governança existentes são inadequadas para o poder e o ritmo do desenvolvimento da IA. O que é particularmente notável é quão rapidamente as preocupações escalaram - de se concentrar principalmente em viés e desinformação no início de 2023 para alertas mais existenciais até 2025, indicando capacidades aceleradas além até mesmo das expectativas dos especialistas.