# Roadmap para aplicativo de streaming de áudio (microfone + áudio interno) no Android

## 1. Viabilidade técnica da captura de áudio interno  
- *Android 10+: A partir do Android 10 (API nível 29) foi introduzida a **AudioPlaybackCapture API, que permite capturar o áudio reproduzido por outros apps (como música ou vídeo) ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=In%20Android%20Q%20there%27s%20a,easier%20content%20sharing%20and%20accessibility)). Esse recurso exige que o app solicitante chame MediaProjectionManager.createScreenCaptureIntent() e obtenha o consentimento do usuário para gravar a tela/áudio ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=Additionally%2C%20before%20a%20capture%20session,Image%3A%20screen%20capture%20intent%20dialog)). Também é preciso declarar no manifest a permissão RECORD_AUDIO e implementar a captura dentro de um *foreground service tipado para mediaProjection ([Android 10 — Audio Capture. Android 10 launched with an Audio… | by Vairavan Srinivasan | Medium](https://medium.com/@debuggingisfun/android-10-audio-capture-77dd8e9070f9#:~:text=App%20needs%20the%20following%20permission,to%20be%20requested%20at%20runtime)) ([Android 10 — Audio Capture. Android 10 launched with an Audio… | by Vairavan Srinivasan | Medium](https://medium.com/@debuggingisfun/android-10-audio-capture-77dd8e9070f9#:~:text=It%20needs%20a%20foreground%20service,mediaProjection)).  
- *Restrições e permissões: Durante a captura, o sistema exibe um ícone de gravação na barra de status (indicando que áudio/vídeo estão sendo capturados) ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=Additionally%2C%20before%20a%20capture%20session,Image%3A%20screen%20capture%20intent%20dialog)). Além disso, por padrão *aplicativos de terceiros não conseguem capturar áudio de outros apps em dispositivos Android 9 ou anteriores – só a partir do target API 29 em diante isso é permitido para usos de mídia/game（AudioAttributes.USAGE_MEDIA,USAGE_GAME） ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=Target%20API%20Third%20party%20apps,type%20MEDIA%2C%20GAME%20and%20UNKNOWN)). Em APIs anteriores, só seria possível com soluções de baixo nível (por exemplo, capturar som do alto-falante via microfone, o que degrada muito a qualidade) ou em dispositivos rooted.  
- *Compatibilidade de perfil: A captura de áudio interno exige que o aplicativo capturador e o app tocando áudio estejam no *mesmo perfil de usuário ([Capture video and audio playback  |  Android media  |  Android Developers](https://developer.android.com/media/platform/av-capture#:~:text=MediaProjectionManager,in%20the%20same%20user%20profile)). Isso impede, por exemplo, capturar áudio de apps rodando em outro usuário ou perfil corporativo. Também é possível que aplicativos bloqueiem sua captura definindo allowAudioPlaybackCapture="false" no manifest ou via AudioAttributes.setAllowedCapturePolicy() ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=Disallowing%20capture%20of%20all%20audio,by%20third%20party%20apps)); nesses casos você não conseguirá captar o áudio desse app específico.  
- *Resumo de viabilidade: Em resumo, só **Android 10 ou superior* garantem captura de áudio interno de forma oficial. Antes disso, *não há API pública confiável* (já que o Google bloqueou gravação de áudio interno nas versões 7–9, exigindo soluções de hardware ou root). Portanto o roadmap deve assumir *minSdkVersion ≥ 21* (para MediaProjection) e *targetSdkVersion ≥ 29* para acesso à AudioPlaybackCapture. Em Android <29, o app deve continuar disponível mas sem a função de áudio interno (por exemplo, capturando apenas o microfone).

## 2. Tecnologias e bibliotecas para streaming em tempo real  
- *RTMP (Real Time Messaging Protocol): Protocolo clássico para streaming ao vivo, originado com Adobe Flash. Em geral, oferece **latência moderada* (tipicamente 2–5 segundos em condições normais) e ampla adoção em servidores de mídia. A implementação costuma ser *relativamente simples* graças a várias bibliotecas maduras (por exemplo, [RootEncoder/rtmp-rtsp-stream-client-java] que suporta RTMP/RTSP/SRT em Java/Kotlin ([GitHub - pedroSG94/RootEncoder: RootEncoder for Android (rtmp-rtsp-stream-client-java) is a stream encoder to push video/audio to media servers using protocols RTMP, RTSP, SRT and UDP with all code written in Java/Kotlin](https://github.com/pedroSG94/RootEncoder#:~:text=RootEncoder%20for%20Android%20%28rtmp,all%20code%20written%20in%20Java%2FKotlin))). Exige um servidor RTMP (como Wowza, nginx-rtmp, Red5, SRS etc.) para ingestão. Prós: suporte maduro, muitas ferramentas; Contras: latência não ultrabaixa, e não é nativo em navegadores modernos (precisa de players específicos).  
- *WebRTC: Framework para comunicação em tempo real baseado em peer-to-peer. Projetado para **latência mínima* (<1 segundo), usando codecs de alta eficiência (Opus para áudio) e protocolos ICE/STUN/TURN para contornar firewalls. Em celulares Android é possível usar a Android WebRTC SDK (p.ex. biblioteca oficial do Google) para capturar e transmitir mídia via WebRTC. Vantagens: interatividade em tempo real, adaptativo à largura de banda, sem servidor de mídia intermediário obrigatoriamente (embora normalmente se use um SFU ou MCU para distribuir). Desvantagens: implementação complexa (é preciso gerenciar sinalização, ICE, negociações SDP etc.) e depende de infraestrutura WebRTC (servidores STUN/TURN). Ideal para cenários de baixa latência e potencial comunicação bidirecional.  
- *Sockets personalizados (TCP/UDP/WebSockets): Enviar áudio puro ou codificado via sockets customizados. Por exemplo, usar um Socket TCP ou DatagramSocket UDP para encaminhar buffers de áudio codificado (AAC/Opus) a um servidor próprio. **Latência: depende muito da implementação e das otimizações (teoricamente pode ser baixa se via UDP+RTP). **Complexidade*: muito alta, pois você precisa lidar manualmente com codificação/packetização, sincronização, perda de pacotes, reconexão, etc. Prós: controle total e flexibilidade (poder otimizar algoritmos próprios); Contras: praticamente “reinventar a roda” de streaming, maior risco de bugs e compatibilidade. Normalmente só vale a pena se você tiver requisitos muito específicos sem atender pelos protocolos acima.  
- *Comparativo em tabela*: Para resumir, veja a tabela abaixo comparando RTMP, WebRTC e sockets customizados em aspectos-chave:

  | Tecnologia     | Latência típica    | Complexidade e requisitos        | Vantagens principais                   | Desvantagens                           |
  |---------------|--------------------|----------------------------------|----------------------------------------|----------------------------------------|
  | *RTMP*      | ~2–5 segundos      | Moderada; requer servidor RTMP.  | Maturidade e bibliotecas disponíveis (ex. RootEncoder) ([GitHub - pedroSG94/RootEncoder: RootEncoder for Android (rtmp-rtsp-stream-client-java) is a stream encoder to push video/audio to media servers using protocols RTMP, RTSP, SRT and UDP with all code written in Java/Kotlin](https://github.com/pedroSG94/RootEncoder#:~:text=RootEncoder%20for%20Android%20%28rtmp,all%20code%20written%20in%20Java%2FKotlin)); fácil ingestão em servidores CDN. | Latência não ultra-baixa; depende de servidor intermediário. |
  | *WebRTC*    | <1 segundo         | Alta; requer sinalização (STUN/TURN) e infraestrutura. | Latência muito baixa; adaptativo; codecs modernos (Opus); bidirecional. | Implementação mais complexa; pouco usado para broadcast unidirecional sem servidor dedicados. |
  | *Sockets*   | Depende (potencialmente muito baixa) | Muito alta; tudo “do zero” (codificação, protocolos, correção de erros). | Controle total de pacote e transporte; flexível para formatos próprios. | Desenvolvimento custoso; responsabilidade de lidar com transmissão segura e estável. |

- *Bibliotecas e exemplos: Para RTMP, destacam-se bibliotecas como [RootEncoder/rtmp-rtsp-stream-client-java] (Java/Kotlin) ([GitHub - pedroSG94/RootEncoder: RootEncoder for Android (rtmp-rtsp-stream-client-java) is a stream encoder to push video/audio to media servers using protocols RTMP, RTSP, SRT and UDP with all code written in Java/Kotlin](https://github.com/pedroSG94/RootEncoder#:~:text=RootEncoder%20for%20Android%20%28rtmp,all%20code%20written%20in%20Java%2FKotlin)) ou [StreamPack](https://github.com/ThibaultBee/StreamPack) com extensão RTMP. Por exemplo, a biblioteca StreamPack suporta fontes de áudio de *microfone ou *“device audio”* (áudio interno) ([GitHub - ThibaultBee/StreamPack: Multiprotocol (SRT, RTMP and others) live streaming libraries for Android](https://github.com/ThibaultBee/StreamPack#:~:text=,audio%20or%20custom%20audio%20source)), facilitando a integração de ambas as fontes. Para WebRTC, pode-se usar o SDK do WebRTC (por exemplo org.webrtc do projeto Chromium) ou soluções como [Pion](https://github.com/pion/) via NDK. Em sockets, uma abordagem simples seria usar Socket/DatagramSocket padrão Java junto com codificadores (como MediaCodec ou bibliotecas JNI de Opus), porém há poucos frameworks prontos. De forma geral, *RTMP* é indicado se a latência de alguns segundos é aceitável e quer rapidez na implementação; *WebRTC* é indicado para baixa latência e interatividade; sockets personalizados só se nenhuma outra solução atender.  

## 3. Permissões do Android e implicações de segurança  
- *Permissões essenciais: O app deve declarar e solicitar em tempo de execução android.permission.RECORD_AUDIO para acesso ao microfone ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=What%20does%20the%20user%20see%3F)). Adicionalmente, para capturar o áudio interno via MediaProjection não há permissão dedicada no manifest além de RECORD_AUDIO, mas deve-se iniciar a *intenção de captura de tela do sistema (MediaProjectionManager.createScreenCaptureIntent()) e aguardar o consentimento do usuário ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=Additionally%2C%20before%20a%20capture%20session,Image%3A%20screen%20capture%20intent%20dialog)). Também inclua a permissão android.permission.FOREGROUND_SERVICE no manifest (necessária em Android 9+ para iniciar serviços em primeiro plano) ([Android 10 — Audio Capture. Android 10 launched with an Audio… | by Vairavan Srinivasan | Medium](https://medium.com/@debuggingisfun/android-10-audio-capture-77dd8e9070f9#:~:text=App%20needs%20the%20following%20permission,to%20be%20requested%20at%20runtime)). A permissão android.permission.INTERNET é obrigatória para streaming ao servidor remoto. Opcionalmente, WAKE_LOCK pode ser usada para manter CPU acordado durante transmissão.  
- *Serviço em primeiro plano: A captura de áudio interno e streaming devem ocorrer em um *Foreground Service para evitar que o sistema mate o processo em background. No Android 9 em diante, deve-se declarar o tipo de serviço como mediaProjection (por exemplo, <service android:foregroundServiceType="mediaProjection">) ([Android 10 — Audio Capture. Android 10 launched with an Audio… | by Vairavan Srinivasan | Medium](https://medium.com/@debuggingisfun/android-10-audio-capture-77dd8e9070f9#:~:text=It%20needs%20a%20foreground%20service,mediaProjection)). Isso garante ao usuário a visualização de notificação permanente de que a gravação está ativa (ícone de tela em vermelho ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=Additionally%2C%20before%20a%20capture%20session,Image%3A%20screen%20capture%20intent%20dialog))).  
- *Consentimento do usuário*: Ao chamar createScreenCaptureIntent(), o Android exibe uma caixa de diálogo (“Start now”) que o usuário deve confirmar ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=Additionally%2C%20before%20a%20capture%20session,Image%3A%20screen%20capture%20intent%20dialog)). Sem essa confirmação, não é possível obter o token de projeção de mídia. Após concedido, fica ativo o ícone vermelho na barra de status, e o usuário pode interromper manualmente a captura a qualquer momento. O app deve tratar o caso de cancelamento (“onActivityResult” sem consentimento) e comportar-se adequadamente.  
- *Implicações de segurança: A gravação de áudio interno é considerada sensível, pois pode capturar áudio de qualquer app. Por isso o sistema impõe as restrições acima. Além disso, aplicativos tocando áudio podem optar por **bloquear* a captura de seus conteúdos (por exemplo, apps de streaming de vídeo/áudio protegido geralmente fazem isso para evitar pirataria). Se um app bloquear, seu áudio não aparecerá nas capturas. Em resumo, informe ao usuário por que o áudio interno está sendo capturado (transparência) e garanta uma comunicação segura com o servidor (por exemplo, use TLS se aplicável) para proteger a privacidade dos dados transmitidos.  

## 4. Melhores práticas para latência e qualidade  
- *Formato e taxa de amostragem: Use taxa de amostragem alta (44.1 kHz ou 48 kHz) e codificação PCM de 16 bits para preservar qualidade do áudio interno (que geralmente é estéreo musical). Para o microfone, considere reduzir para mono se o aplicativo for apenas voz, para poupar banda. O buffer mínimo (AudioRecord.getMinBufferSize) deve ser calculado e um pouco maior que o mínimo ideal, para evitar *underflows.  
- *Codificação eficiente*: Antes de enviar, comprima o áudio usando codecs modernos (por exemplo, AAC para RTMP ou Opus para WebRTC). O Android fornece MediaCodec para AAC no SDK, e bibliotecas NDK para Opus. Usar codecs em hardware (via MediaCodec) reduz a carga da CPU. Evite codificar várias vezes – capture diretamente no formato final ou o mais próximo possível.  
- *Buffers e threads: Mantenha buffers de áudio pequenos para reduzir latência end-to-end, mas grandes o suficiente para estabilidade. Use threads separadas (ou *AudioThread/Executors) para captura de áudio e envio em rede, evitando bloqueios no render thread principal. Configure prioridade de thread maior para áudio se disponível.  
- *Gerenciamento de rede: Prefira protocolos UDP (como WebRTC faz) se a rede for instável, pois evitam retransmissão de dados e quedas de latência. Se usar TCP (como RTMP/HTTP), ajuste o tamanho dos pacotes e buffers de socket para reduzir *jitter. Monitore alterações de rede (3G/4G/Wi-Fi) e adapte o bitrate de áudio conforme necessário.  
- *Áudio do dispositivo: Certifique-se de pedir captura de *USAGE_MEDIA e USAGE_GAME ao configurar AudioPlaybackCaptureConfiguration, pois isso inclui sons de apps de mídia ([Capture video and audio playback  |  Android media  |  Android Developers](https://developer.android.com/media/platform/av-capture#:~:text=To%20capture%20audio%20from%20another,Follow%20these%20steps)). Para latência extra baixa no microfone, considere usar a API AAudio (Android 8.0+) ou a biblioteca Oboe, que são otimizadas para tempo real. Ajuste AudioRecord para usar AudioSource.VOICE_COMMUNICATION ou UNPROCESSED (API 24+) para evitar processamento extra que aumente o atraso.  
- *Sincronização e multiplexação*: Como haverá dois canais (microfone e interno), mantenha estampa temporal ou sincronize buffers na origem se necessário. Envie-os separadamente ou multiplexados conforme suporte do servidor. No servidor, é possível mesclar os canais se desejar mono mix ou lidar como streams independentes. Minimize encoding entre canais – por exemplo, não reconverta novamente de PCM para PCM – e envie rapidamente após capturar.  

## 5. Estrutura do aplicativo (arquitetura e camadas)  
- *Módulos principais: Separe o app em camadas/funções claras: (a) *Interface de usuário (Activity/Fragment) para controlar início/stop do streaming e mostrar status; (b) Serviço de captura (ForegroundService) que gerencia MediaProjection e AudioRecord para ambos os canais; (c) Módulo de streaming responsável por empacotar os dados de áudio codificados e enviá-los ao servidor; (d) possivelmente um Layer de rede ou cliente para RTMP/WebRTC/Sockets. Use padrões de projeto (por exemplo, injeção de dependência) para desacoplar as camadas de áudio das de rede.  
- *Serviço em primeiro plano: Implemente um Service que, quando iniciado, solicita permissões (e.g. chamada a MediaProjectionManager) e exibe notificação contínua. Esse serviço deve criar **duas instâncias de captura de áudio*: uma AudioRecord convencional para o microfone (MediaRecorder.AudioSource.MIC) e outra com AudioPlaybackCaptureConfiguration para o áudio interno ([Capture video and audio playback  |  Android media  |  Android Developers](https://developer.android.com/media/platform/av-capture#:~:text=To%20capture%20audio%20from%20another,Follow%20these%20steps)). Cada instância roda em sua própria thread de captura para não bloquear a outra.  
- *Streaming independente: Para canal duplo, você pode: (1) codificar separadamente cada stream e enviá-los como dois fluxos distintos (se o servidor suportar multiplos *inputs simultâneos); ou (2) mesclar internamente se só for preciso um stream único (por exemplo, mixar áudio interno + microfone). A abordagem preferível é *fluxos separados, mantendo flexibilidade. Por exemplo, usando RTMP, você poderia iniciar duas conexões (uma publicando o áudio do mic, outra o interno) ou usar metadados/contratexto para distinguir. Em WebRTC, pode-se adicionar duas *trilhas de áudio distintas num mesmo PeerConnection.  
- *Tratamento de estado: Use *broadcast receivers ou mecanismos de callback (LiveData, listeners) para notificar a UI sobre eventos (ex.: permissão negada, conexão perdida, token de MediaProjection expirada). Garanta que o serviço gerencie adequadamente situações como: perda de conexão de rede (tentar reconnect), encerramento pelo usuário (limpar recursos), ou interrupção do MediaProjection (usuário cancelou captura).  
- *Boa prática de design*: Adote MVVM ou MVP na camada de UI, mas mantenha lógica pesada (captura e rede) fora das Activities. Considere usar ViewModel para manter estado da gravação entre rotações. Ao iniciar streaming, inicialize os componentes de áudio e rede em sequência, validando permissões. Documente claramente no código as responsabilidades de cada classe (por exemplo, AudioCaptureService cuida só da captação bruta e codificação, enquanto StreamingClient cuida do protocolo de envio).  

## 6. Compatibilidade com versões do Android  
- *Microfone (capture comum)*: Para capturar do microfone, o mínimo é Android 6.0 (API 23) para ter o modelo de permissão em tempo de execução; porém a API AudioRecord está disponível desde muito antes. Em versões antigas (pré-6.0), basta declarar a permissão no manifest (não tem runtime). Se definir minSdkVersion ≥ 23, trate runtime do RECORD_AUDIO.  
- *Áudio interno: Apenas **API 29+*. Se for necessário dar suporte a dispositivos com Android 28 ou inferior, o app pode simplesmente desabilitar essa funcionalidade (por exemplo, uma mensagem “Áudio interno não disponível”). Não é recomendável usar hacks (microfone no alto-falante) devido à qualidade ruim. Como alternativa, se houver requisitos críticos, investigue APIs proprietárias de fabricantes (ex.: Samsung Game Tools tem recurso interno de gravação para jogos), mas isso sai do escopo padrão.  
- *MediaProjection*: Requer API 21+ para funcionar. Em Android 21–28, você pode capturar tela (vídeo) mas não há áudio interno oficial. A API de captura de tela deve ser usada somente para acionar o prompt de consentimento em 29+.  
- *Foreground Service: A partir do Android 8.0 (API 26), serviços em segundo plano têm restrições severas; por isso **sempre* use foreground service para streaming contínuo, e crie um canal de notificação (obrigatório no API 26+). Em Android 9+ declare <uses-permission android:name="android.permission.FOREGROUND_SERVICE"/> mesmo que essa permissão seja “normal” (não controla privacidade, mas o sistema exige declará-la para startForegroundService) ([Android 10 — Audio Capture. Android 10 launched with an Audio… | by Vairavan Srinivasan | Medium](https://medium.com/@debuggingisfun/android-10-audio-capture-77dd8e9070f9#:~:text=App%20needs%20the%20following%20permission,to%20be%20requested%20at%20runtime)).  
- *Permissões e alterações recentes: No Android 11/12 (API 30/31), não há mudanças diretas para áudio interno, mas regras de background e de *“restart”* de serviços podem afetar: garanta tratar onStartCommand retornando START_STICKY se quiser reiniciar. No Android 13 (API 33), entrou a permissão de microfone em grupo de privacidade (não altera para apps de gravação comuns). Verifique também ajustes de *Battery Optimizations que possam interromper serviços de longa duração; considere isentar o app se necessário (via ACTION_REQUEST_IGNORE_BATTERY_OPTIMIZATIONS).  
- *Resumos por versão*:   
  - API 21–28: Gravação de microfone e captura de tela possível, mas *não captura áudio interno* de outros apps.  
  - API 29 (Android 10): Introduz AudioPlaybackCapture. É possível capturar áudio interno com consentimento. Apps podem optar por bloquear captura nos manifestos antigos.  
  - API 30+: Mesmo padrão de captura de áudio. Atenção especial a políticas de background e ao gerenciamento de serviços.  

## 7. Exemplos de código (Android)  

- *Captura de áudio (microfone e interno)*: Abaixo um exemplo ilustrativo de como criar duas instâncias AudioRecord, uma para o mic e outra para áudio interno via AudioPlaybackCaptureConfiguration (apenas API 29+). Note a parte de configuração do MediaProjection (mediaProjection deve ser obtido após o usuário aceitar o prompt).  

  java
  // Configuração geral
  int sampleRate = 44100;
  int channelConfig = AudioFormat.CHANNEL_IN_STEREO;
  int audioFormat = AudioFormat.ENCODING_PCM_16BIT;
  int minBufSize = AudioRecord.getMinBufferSize(sampleRate, channelConfig, audioFormat);

  // 1) Captura do microfone
  AudioRecord micRecorder = new AudioRecord(
      MediaRecorder.AudioSource.MIC,
      sampleRate, channelConfig,
      audioFormat, minBufSize
  );
  micRecorder.startRecording();

  // 2) Captura do áudio interno (exige API >= 29 e MediaProjection)
  AudioPlaybackCaptureConfiguration config = 
      new AudioPlaybackCaptureConfiguration.Builder(mediaProjection)
          .addMatchingUsage(AudioAttributes.USAGE_MEDIA)
          .build();
  AudioRecord internalRecorder = new AudioRecord.Builder()
      .setAudioFormat(new AudioFormat.Builder()
          .setEncoding(audioFormat)
          .setSampleRate(sampleRate)
          .setChannelMask(channelConfig)
          .build())
      .setBufferSizeInBytes(minBufSize)
      .setAudioPlaybackCaptureConfig(config)
      .build();
  internalRecorder.startRecording();
  

  Após esses startRecording(), os buffers de áudio podem ser lidos com read() de cada AudioRecord e enviados ao codificador/streaming. O uso de addMatchingUsage(AudioAttributes.USAGE_MEDIA) garante captura de apps de mídia ([Capture video and audio playback  |  Android media  |  Android Developers](https://developer.android.com/media/platform/av-capture#:~:text=To%20capture%20audio%20from%20another,Follow%20these%20steps)).  

- *Solicitação de permissões*: Para gravar do microfone, peça RECORD_AUDIO no tempo de execução (por exemplo, via requestPermissions). Para captura de tela/áudio interno, acione:  
  java
  MediaProjectionManager mgr = (MediaProjectionManager) getSystemService(Context.MEDIA_PROJECTION_SERVICE);
  startActivityForResult(mgr.createScreenCaptureIntent(), REQUEST_MEDIA_PROJECTION);
  
  e em onActivityResult obtenha o mediaProjection a partir do resultCode e data. Esse fluxo exibirá ao usuário o diálogo de consentimento ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=Additionally%2C%20before%20a%20capture%20session,Image%3A%20screen%20capture%20intent%20dialog)).  

- *Exemplo de biblioteca RTMP*: Usando a biblioteca RootEncoder, poderia-se fazer:  
  java
  RtmpPublisher publisher = new RtmpPublisher("rtmp://example.com/live/streamKey");
  publisher.addAudioTrack(micRecorder);       // enviar áudio do microfone
  publisher.addAudioTrack(internalRecorder);  // enviar áudio interno
  publisher.start();
    
  (API hipotética; ilustrativa). A ideia é que a lib consuma os buffers PCM dos AudioRecord e os envie via RTMP. A biblioteca suporta múltiplas fontes de áudio ([GitHub - pedroSG94/RootEncoder: RootEncoder for Android (rtmp-rtsp-stream-client-java) is a stream encoder to push video/audio to media servers using protocols RTMP, RTSP, SRT and UDP with all code written in Java/Kotlin](https://github.com/pedroSG94/RootEncoder#:~:text=RootEncoder%20for%20Android%20%28rtmp,all%20code%20written%20in%20Java%2FKotlin)).  

Este roadmap cobre desde a *viabilidade* e *restrições de API/segurança* até a *implementação prática* (bibliotecas e códigos) para transmitir simultaneamente áudio de microfone e áudio interno. Cada etapa deve ser validada em testes, especialmente considerando as diferenças de comportamento entre versões do Android. 

*Fontes:* Documentação oficial Android sobre captura de áudio interno ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=In%20Android%20Q%20there%27s%20a,easier%20content%20sharing%20and%20accessibility)) ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=Additionally%2C%20before%20a%20capture%20session,Image%3A%20screen%20capture%20intent%20dialog)) ([Android Developers Blog:  Capturing Audio in Android Q](https://android-developers.googleblog.com/2019/07/capturing-audio-in-android-q.html#:~:text=Target%20API%20Third%20party%20apps,type%20MEDIA%2C%20GAME%20and%20UNKNOWN)); exemplos de código (AudioPlaybackCapture API) ([Capture video and audio playback  |  Android media  |  Android Developers](https://developer.android.com/media/platform/av-capture#:~:text=To%20capture%20audio%20from%20another,Follow%20these%20steps)); bibliotecas de streaming (RootEncoder RTMP) ([GitHub - pedroSG94/RootEncoder: RootEncoder for Android (rtmp-rtsp-stream-client-java) is a stream encoder to push video/audio to media servers using protocols RTMP, RTSP, SRT and UDP with all code written in Java/Kotlin](https://github.com/pedroSG94/RootEncoder#:~:text=RootEncoder%20for%20Android%20%28rtmp,all%20code%20written%20in%20Java%2FKotlin)) e StreamPack ([GitHub - ThibaultBee/StreamPack: Multiprotocol (SRT, RTMP and others) live streaming libraries for Android](https://github.com/ThibaultBee/StreamPack#:~:text=,audio%20or%20custom%20audio%20source)); guias de permissões no Android ([Android 10 — Audio Capture. Android 10 launched with an Audio… | by Vairavan Srinivasan | Medium](https://medium.com/@debuggingisfun/android-10-audio-capture-77dd8e9070f9#:~:text=App%20needs%20the%20following%20permission,to%20be%20requested%20at%20runtime)) ([Android 10 — Audio Capture. Android 10 launched with an Audio… | by Vairavan Srinivasan | Medium](https://medium.com/@debuggingisfun/android-10-audio-capture-77dd8e9070f9#:~:text=It%20needs%20a%20foreground%20service,mediaProjection)).