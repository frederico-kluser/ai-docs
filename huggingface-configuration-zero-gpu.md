# üöÄ Guia Completo para ZeroGPU no Hugging Face Spaces

## 1. üìñ O que √© ZeroGPU e Por que √© Diferente?

O **ZeroGPU** √© uma infraestrutura de GPU compartilhada do Hugging Face que gerencia dinamicamente recursos de GPU para
aplica√ß√µes de Machine Learning. A principal inova√ß√£o est√° em sua abordagem **"serverless"** ou sob demanda.

### üîÑ Arquitetura Tradicional vs. ZeroGPU

| Caracter√≠stica       | GPU Tradicional (T4, A10G, etc.)                              | ZeroGPU (H200 slice)                                                                              |
| :------------------- | :------------------------------------------------------------ | :------------------------------------------------------------------------------------------------ |
| **Aloca√ß√£o**         | GPU dedicada e reservada 24/7 enquanto o Space estiver ativo. | GPU alocada apenas quando uma fun√ß√£o decorada √© executada e liberada ap√≥s.                        |
| **Custo**            | Cobran√ßa por minuto de _runtime_, independente de uso.        | Acesso gratuito para uso, com cotas di√°rias de minutos. Hosting requer assinatura PRO/Enterprise. |
| **Modelo Econ√¥mico** | Pay-as-you-go (pode ficar caro).                              | Custo fixo para criadores (via assinatura), extremamente custo-efetivo para modelos grandes.      |
| **Hardware**         | Variado (T4, L4, A100, etc.).                                 | Fatia (_slice_) de uma NVIDIA **H200** com **70GB de VRAM** por workload.                         |
| **Multi-GPU**        | Configura√ß√£o est√°tica e fixa (ex: 4x L4).                     | Suporte a m√∫ltiplas GPUs concorrentes para uma √∫nica aplica√ß√£o.                                   |

**O Princ√≠pio Fundamental**: No ZeroGPU, a GPU **n√£o est√° dispon√≠vel** quando seu aplicativo inicia. O processo
principal roda em CPU. Apenas quando uma fun√ß√£o espec√≠fica (decorada com `@spaces.GPU`) √© chamada, o sistema cria um
processo "fork", aloca uma GPU para ele, executa a fun√ß√£o e finaliza o fork. Por isso, qualquer tentativa de acessar a
GPU (como `model.to('cuda')`) **fora** de uma fun√ß√£o decorada causar√° falha.

## 2. ‚öôÔ∏è Como Implementar Corretamente

### üì¶ Pr√©-requisitos e Configura√ß√£o

1.  **Hardware do Space**: No menu de configura√ß√µes (_Settings_) do seu Space, o hardware deve ser **"ZeroGPU"**.
2.  **SDK Suportado**: Apenas **Gradio** (vers√£o 4+).
3.  **Vers√µes Compat√≠veis**:
    - **Python**: 3.10.13.
    - **PyTorch**: Vers√µes da 2.1.0 at√© a mais recente s√£o suportadas. Consulte a
      [documenta√ß√£o oficial](https://huggingface.co/docs/hub/en/spaces-zerogpu) para a lista completa.
4.  **Requirements.txt**: Para garantir uma instala√ß√£o compat√≠vel com CUDA, use:
    ```txt
    --extra-index-url https://download.pytorch.org/whl/cu121
    torch>=2.1.0
    transformers
    gradio>=4.0.0
    ```
    A flag `--extra-index-url` √© crucial para obter os bin√°rios corretos do PyTorch.

### üß† Padr√£o de C√≥digo Essencial

A regra de ouro: **NUNCA carregue o modelo ou chame qualquer opera√ß√£o CUDA no escopo global do `app.py`**.

```python
# CORRETO ‚úÖ
import gradio as gr
import spaces
import torch

# 1. Importe a biblioteca spaces
import spaces

# 2. N√ÉO carregue o modelo aqui. Defina vari√°veis vazias ou estado.
model = None
tokenizer = None

@spaces.GPU
def load_model_once():
    """Fun√ß√£o para carregar o modelo dentro do contexto GPU."""
    global model, tokenizer
    if model is None:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained("seu-modelo")
        model = AutoModelForCausalLM.from_pretrained("seu-modelo")
        model.to('cuda')  # SEGURO: dentro da fun√ß√£o decorada
    return model, tokenizer

@spaces.GPU
def generate(prompt):
    """Fun√ß√£o principal de infer√™ncia."""
    # Carrega o modelo na primeira chamada
    local_model, local_tokenizer = load_model_once()
    # Realiza a infer√™ncia
    inputs = local_tokenizer(prompt, return_tensors="pt").to('cuda')
    with torch.no_grad():
        outputs = local_model.generate(**inputs)
    return local_tokenizer.decode(outputs[0])

# Interface Gradio
iface = gr.Interface(fn=generate, inputs="text", outputs="text")
iface.launch()
```

**Erro Comum (que vivenciamos)** ‚ùå:

```python
# ERRADO ‚ùå - Causa "No @spaces.GPU function detected"
import spaces
from transformers import pipeline

# O pipeline tenta ir para 'cuda' no escopo global, antes de qualquer @spaces.GPU
pipe = pipeline("text-to-speech", "suno/bark", device="cuda")

@spaces.GPU
def generate(text):
    return pipe(text)  # Tarde demais! A GPU j√° falhou ao inicializar.
```

### ‚è± Gerenciamento de Dura√ß√£o e Filas

Fun√ß√µes decoradas com `@spaces.GPU` t√™m um limite padr√£o de **60 segundos**. Para tarefas mais longas, defina uma
dura√ß√£o m√°xima:

```python
@spaces.GPU(duration=120)  # M√°ximo de 120 segundos
def generate_long(prompt, steps=50):
    # ... gera√ß√£o lenta ...
    return result
```

**Importante**: Especificar uma dura√ß√£o **menor e mais realista** para suas fun√ß√µes d√° **maior prioridade na fila** de
espera do ZeroGPU para os visitantes do seu Space. O uso √© regido por cotas di√°rias:

| Tipo de Conta   | Cota Di√°ria de GPU | Prioridade na Fila |
| :-------------- | :----------------- | :----------------- |
| N√£o autenticado | 2 minutos          | Baixa              |
| Conta Gratuita  | 3.5 minutos        | M√©dia              |
| **PRO / Team**  | **25 minutos**     | **Mais Alta**      |
| Enterprise      | 45 minutos         | Mais Alta          |

## 3. üöÄ Otimiza√ß√£o de Performance

### üõ† Compila√ß√£o "Ahead-of-Time" (AoT)

Como o ZeroGPU cria um novo processo para cada tarefa, a compila√ß√£o JIT padr√£o (`torch.compile`) n√£o √© eficiente, pois
precisaria recompilar a cada vez. A solu√ß√£o √© a **compila√ß√£o antecipada**:

```python
import spaces
import torch

@spaces.GPU(duration=300)  # Compila√ß√£o pode ser demorada
def compile_model():
    # 1. Capturar entradas de exemplo
    with spaces.aoti_capture(model) as call:
        dummy_output = model(dummy_input)
    # 2. Exportar o modelo
    exported = torch.export.export(model, args=call.args)
    # 3. Compilar
    compiled_model = spaces.aoti_compile(exported)
    return compiled_model

# Compila uma vez e usa muitas vezes
compiled = compile_model()
spaces.aoti_apply(compiled, model)  # Substitui o 'forward' do modelo
```

Essa t√©cnica pode acelerar a infer√™ncia em **1.3x a 1.8x**.

### üíæ Gerenciamento de Estado com Cache

Para evitar recarregar o modelo em toda chamada (dentro do mesmo processo), use cache:

```python
from functools import lru_cache

@spaces.GPU
@lru_cache(maxsize=1)  # Cache dentro do processo
def get_model():
    model = AutoModel.from_pretrained("meu-modelo").to('cuda')
    return model

@spaces.GPU
def predict(prompt):
    model = get_model()  # Carrega apenas na primeira chamada do processo
    return model.generate(prompt)
```

## 4. üêõ Solu√ß√£o de Problemas Comuns

| Problema                                 | Causa Prov√°vel                                                                         | Solu√ß√£o                                                                                                           |
| :--------------------------------------- | :------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------- |
| **`"No @spaces.GPU function detected"`** | O c√≥digo tentou acessar `'cuda'` antes da primeira fun√ß√£o decorada ser chamada.        | Garanta que **todo** acesso a CUDA (`.to('cuda')`, `torch.cuda.*`) esteja **dentro** de uma fun√ß√£o `@spaces.GPU`. |
| **`"Can't initialize NVML"`**            | O PyTorch tentou inicializar a GPU no processo principal (CPU).                        | √â um **sintoma** do problema acima, n√£o a causa raiz. Corrija o acesso prematuro √† GPU.                           |
| **Modelo n√£o carrega / OOM**             | O processo fork tem 70GB, mas o modelo pode ser grande.                                | Use quantiza√ß√£o (`.to(torch.float16)`), carregue com `device_map="auto"` ou otimize com AoT.                      |
| **Lat√™ncia alta na primeira chamada**    | Carregamento do modelo do disco.                                                       | Use o padr√£o de cache mostrado acima. A compila√ß√£o AoT tamb√©m ajuda.                                              |
| **Erro de Timeout ( >60s )**             | A fun√ß√£o excedeu o limite padr√£o.                                                      | Use o par√¢metro `@spaces.GPU(duration=...)` para aumentar o limite.                                               |
| **Space trava ao usar Client API**       | Requisi√ß√µes program√°ticas n√£o passam o token de usu√°rio, esgotando a cota rapidamente. | Extraia e passe o header `X-IP-Token` da request do usu√°rio final para o Client Gradio.                           |

## 5. ‚úÖ Checklist para Implanta√ß√£o

Antes de publicar seu Space ZeroGPU, confirme:

1.  [ ] **Hardware** est√° configurado como "ZeroGPU" nas Settings.
2.  [ ] **Nenhuma opera√ß√£o CUDA** (`to('cuda')`, `torch.cuda.*`) no escopo global.
3.  [ ] **Todas as fun√ß√µes** que usam GPU est√£o decoradas com `@spaces.GPU`.
4.  [ ] **Dura√ß√£o** (`duration=`) definida para fun√ß√µes que levam mais de ~50s.
5.  [ ] **`requirements.txt`** especifica uma vers√£o compat√≠vel do PyTorch com `--extra-index-url`.
6.  [ ] **Logs de Build** foram verificados para confirmar `torch.cuda.is_available()` √© `True` durante a execu√ß√£o da
        fun√ß√£o decorada.

A principal mentalidade para dominar o ZeroGPU √© internalizar seu **modelo de execu√ß√£o sob demanda**. Uma vez que voc√™
estrutura seu c√≥digo para carregar modelos e executar computa√ß√µes pesadas **exclusivamente** dentro das fun√ß√µes
decoradas `@spaces.GPU`, pode aproveitar uma GPU poderosa (H200) de forma gratuita e eficiente para seus demos.

Para se aprofundar, consulte a [documenta√ß√£o oficial do ZeroGPU](https://huggingface.co/docs/hub/en/spaces-zerogpu) e o
[blog sobre compila√ß√£o AoT](https://huggingface.co/blog/zerogpu-aoti).
